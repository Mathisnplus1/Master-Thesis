QUESTIONS :
- Decide how we choose the threshold for when to grow
- Decide how we choose the threshold for the hessian coefficient we want to considere
- Decide how we initialize the new neurons (bias, weights and gradients)
- Decide what kind of permutation we want to perform (full permutation or just an inside square ?)
- What do we want to freeze ?

REMARKS :
- We grow one time, no problem, but what about the second growth ? Do we use the frozen weights or the grown ones ? (Currently perform the second growth on)
- Makes no sense to compute second derivative of the loss with respect to frozen weights
- Number of new neurons can only decrease between to growth (eventually converges twoards 0 added neurons, which causes the learning process to crash). We could enforce growing at least one neuron ?
- Redundancy : the neurons we add at n+1th growth are a subset of the neurons we grew at nth growth, so in the end, each neuron we grow are "copies" of interesting neurons on task 1
- Currently, we simply use the Hessian information do "copy" neurons, but we don't leverage the backprop's gradient information
- Over-exploitation of fc3 frozen after task 1 (so, they are important and never retrained)
- We lack symmetry between the tasks : this looks like fine tuning, as we heavily rely on the weights trained for task 1