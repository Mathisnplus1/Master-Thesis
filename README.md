# Master-Thesis
Artificial neurogenesis in the context of hardware-constrained continual learning

In this thesis, we introduce and develop a novel approach to artificial neurogenesis in the context of continual learning. It involves the generation and integration of new neural units within an artificial neural network that is being trained, enabling it to adapt and learn without forgetting previous knowledge. After stating the specific hypotheses of our framework and comparing our method to the literature, we introduce a formalism to bring to light the various ways hyperparameters optimizations can be conducted in continual learning. Additionnaly, we motivate and introduce a validation paradigm specific to continual learning that we leverage to demonstrate overfitting on benchmarks. An important part of this work focuses on a critical analysis of a whole branch of the literature, including our own approach. After putting forward the assumptions implicitly made by numerous approaches available in the literature, this critic aims to show how they lead to inherent limitations embedded in their core.
