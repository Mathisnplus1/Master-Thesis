{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "4jVkOWmgFT1p"
   },
   "source": [
    "# **Practical session on Transfer Learning**\n",
    "This Pratical session proposes to study several techniques for improving challenging context, in which few data and resources are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "QLKnIngy_2hg"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "**Context :**\n",
    "\n",
    "Assume we are in a context where few \"gold\" labeled data are available for training, say \n",
    "\n",
    "$$\\mathcal{X}_{\\text{train}} = \\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$$\n",
    "\n",
    "where $N_{\\text{train}}$ is small. \n",
    "\n",
    "A large test set $\\mathcal{X}_{\\text{test}}$ as well as a large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
    "\n",
    "**Instructions to follow :** \n",
    "\n",
    "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question :\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   XXX  | XXX | XXX | XXX |\n",
    "\n",
    "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset!)\n",
    "\n",
    "In your final report, please *keep the logs of each training procedure* you used. We will only run this jupyter if we have some doubts on your implementation. \n",
    "\n",
    "The total file sizes should be reasonable (feasible with 2MB only!). You will be asked to hand in the notebook, together with any necessary files required to run it if any.\n",
    "\n",
    "You can use https://colab.research.google.com/ to run your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "YmTCQPSh_2hg"
   },
   "source": [
    "## Training set creation\n",
    "__Question 1 (1 points) :__ Propose a dataloader to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set.\n",
    "\n",
    "Additional information :  \n",
    "\n",
    "*   CIFAR10 dataset : https://en.wikipedia.org/wiki/CIFAR-10\n",
    "*   You can directly use the dataloader framework from Pytorch.\n",
    "*   Alternatively you can modify the file : https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR samples are PIL images, we use the module transforms to convert them into torch tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Download and load the dataset\n",
    "cifar_train_dataset = datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "\n",
    "# Reduce the dataset to the first 100 samples\n",
    "subset_indices = range(100)\n",
    "X_train = Subset(cifar_train_dataset, subset_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we stick to a dataset object. Later on, we will use DataLoader from torch.utils.data with different values for the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "fUno1nmu_2hh"
   },
   "source": [
    "* This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. \n",
    "\n",
    "* The remaining samples correspond to $\\mathcal{X}$. \n",
    "\n",
    "* The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We defines the other sets\n",
    "X = Subset(cifar_train_dataset, range(100,50000))\n",
    "X_test = datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "Vr0d4o5L_2hi"
   },
   "source": [
    "## Testing procedure\n",
    "__Question 2 (0.5 points):__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 6,\n",
       " '1': 16,\n",
       " '2': 13,\n",
       " '3': 13,\n",
       " '4': 11,\n",
       " '5': 7,\n",
       " '6': 7,\n",
       " '7': 11,\n",
       " '8': 4,\n",
       " '9': 12}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's count how many samples we have in each class for training\n",
    "class_counter = {\"0\" : 0,\n",
    "                 \"1\" : 0,\n",
    "                 \"2\" : 0,\n",
    "                 \"3\" : 0,\n",
    "                 \"4\" : 0,\n",
    "                 \"5\" : 0,\n",
    "                 \"6\" : 0,\n",
    "                 \"7\" : 0,\n",
    "                 \"8\" : 0,\n",
    "                 \"9\" : 0}\n",
    "for x in X_train :\n",
    "    class_counter[str(x[1])] += 1\n",
    "class_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "ppiTrnpd_2hi"
   },
   "source": [
    "Under our assumptions (few labelled data and limited computational budget), evaluating the training procedure can be challenging. In particular, we identify 2 mains problems :\n",
    "1. If don't take care, we are likely to encounter overfitting as they are only few samples\n",
    "2. Unbalanced training dataset. As counted above, certain classes are more represented than others (more training samples)\n",
    "3. We might encounter high variance in results if we initialize the model with different random seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we identify potential solutions for each of these issues :\n",
    "1. Actually, take care (regularization, dropout, early stopping...)\n",
    "2. Several potential solutions :\n",
    "    1. Naively augment the training dataset to make it more balanced\n",
    "    2. Employ a balanced learning loss to provide more weight to samples coming from poorly represented classes\n",
    "    3. Perform some kind of transfert learning to indirectly learn some features from unlabelled samples. We could design an arbitrary task, like asking a Domain Adversarial Neural Network to learn distinguising labelled data and unlabelled data. That could result in the creation of useful features for the main task. \n",
    "3. Several potential solutions :\n",
    "    1. Fix a specific random see\n",
    "    2. Perform deterministic initializaton of the model\n",
    "    3. Train numerous models initialized with various random seeds and report standard deviation of performances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "OEaIwILB_2hi"
   },
   "source": [
    "# The Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "M-PQZ2Vl_2hi"
   },
   "source": [
    "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performance with reported numbers from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
    "\n",
    "The key ingredients for training a CNN are the batch size, as well as the learning rate scheduler (i.e. how to decrease the learning rate as a function of the number of epochs). A possible scheduler is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the learning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
    "\n",
    "You can get some baselines accuracies in this paper (obviously, it is a different context for those researchers who had access to GPUs!) : http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "ARHWPXrY_2hi"
   },
   "source": [
    "## ResNet architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "voMbGoNw_2hj"
   },
   "source": [
    "__Question 3 (2 points) :__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1512.03385). Please report the accuracy obtained on the whole dataset as well as the reference paper/GitHub link.\n",
    "\n",
    "*Hint :* You can re-use the following code : https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (\\~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (\\~5 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really tried our best for the baseline. We even performed hyperparameter optimization with optuna, but it didn't make a big difference, so we cut the search at 3 attempts, in case you would like to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, epochs, X_train_loader, X_test_loader, scheduler=None) :\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss_list = []\n",
    "    model.to(device)\n",
    "    #print(f\"epochs : {epochs}\")\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in X_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        loss_list.append(running_loss / len(X_train_loader))\n",
    "        if scheduler is not None :\n",
    "            print(scheduler.get_last_lr()[0])\n",
    "            scheduler.step()\n",
    "        #print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in X_test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    #print(f\"Accuracy on test set: {test_accuracy}\")\n",
    "    return loss_list, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial) :\n",
    "    p = trial.suggest_float(\"p\", 0., 0.4)\n",
    "    #print(f\"p : {p}\")\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Dropout(p),\n",
    "                             nn.Linear(num_ftrs, 10))\n",
    "    return model\n",
    "\n",
    "def objective(trial) :\n",
    "    model = define_model(trial).to(device)\n",
    "    lr = trial.suggest_categorical(\"lr\", [1e-3, 2e-3, 5e-3])\n",
    "    epochs = trial.suggest_categorical(\"epochs\", [10,20,30,40])\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 4, 40)\n",
    "    \n",
    "    #print(f\"lr : {lr}\")\n",
    "    #print(f\"batch_size : {batch_size}\")\n",
    "    \n",
    "    X_train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "    X_test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_list, test_accuracy = train(model, device, optimizer, epochs, X_train_loader, X_test_loader)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-20 17:54:38,958] A new study created in memory with name: The Search\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.60it/s]\n",
      "[I 2024-03-20 17:54:48,940] Trial 0 finished with value: 0.1594 and parameters: {'p': 0.19964678745865286, 'lr': 0.002, 'epochs': 20, 'batch_size': 24}. Best is trial 0 with value: 0.1594.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 14.79it/s]\n",
      "[I 2024-03-20 17:54:55,066] Trial 1 finished with value: 0.1791 and parameters: {'p': 0.3473525174851586, 'lr': 0.002, 'epochs': 40, 'batch_size': 35}. Best is trial 1 with value: 0.1791.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:09<00:00,  3.12it/s]\n",
      "[I 2024-03-20 17:55:15,277] Trial 2 finished with value: 0.1982 and parameters: {'p': 0.23029903595121448, 'lr': 0.001, 'epochs': 30, 'batch_size': 5}. Best is trial 2 with value: 0.1982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats\n",
      "Number of finished trials :  3\n",
      "Number of pruned trials :  0\n",
      "Number of complete trials :  3\n",
      "Best Trial\n",
      " accuracy :  0.1982\n",
      "Params : \n",
      "p : 0.23029903595121448\n",
      "lr : 0.001\n",
      "epochs : 30\n",
      "batch_size : 5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "storage = optuna.storages.InMemoryStorage()\n",
    "study = optuna.create_study(storage=storage,\n",
    "                            study_name=\"The Search\",\n",
    "                            #sampler=\n",
    "                            direction = \"maximize\")\n",
    "study.optimize(objective,\n",
    "               n_trials=3,\n",
    "               timeout=3600)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.COMPLETE])\n",
    "\n",
    "print(\"Stats\")\n",
    "print(\"Number of finished trials : \", len(study.trials))\n",
    "print(\"Number of pruned trials : \", len(pruned_trials))\n",
    "print(\"Number of complete trials : \", len(complete_trials))\n",
    "\n",
    "print(\"Best Trial\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\" accuracy : \", trial.value)\n",
    "\n",
    "print(\"Params : \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{} : {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [01:38<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "# Report the best parameters\n",
    "p = trial.params[\"p\"]\n",
    "lr = trial.params[\"lr\"]\n",
    "epochs = trial.params[\"epochs\"]\n",
    "batch_size = trial.params[\"batch_size\"]\n",
    "\n",
    "# Set the device\n",
    "device = \"cpu\"\n",
    "\n",
    "# Define the best model\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(nn.Dropout(p),\n",
    "                         nn.Linear(num_ftrs, 10))\n",
    "\n",
    "# Define the best loaders and optimizer\n",
    "X_train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "X_test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "loss_list, test_accuracy = train(model, device, optimizer, epochs, X_train_loader, X_test_loader)\n",
    "\n",
    "# Compute train accuracy\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in X_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "train_accuracy = accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_score(loss_list):\n",
    "    plt.figure(figsize=[10, 5])\n",
    "    plt.plot(range(len(loss_list)), loss_list)\n",
    "    plt.title(\"Evolution of the loss w.r.t epochs\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 1.0\n",
      "Accuracy on test set: 0.202\n"
     ]
    }
   ],
   "source": [
    "# Report results\n",
    "plot_f1_score(loss_list)\n",
    "print(f\"Accuracy on train set: {train_accuracy}\")\n",
    "print(f\"Accuracy on test set: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As found on https://paperswithcode.com/sota/stochastic-optimization-on-cifar-10-resnet-18, ResNet-18 generally obtains 95% accuracy on the test set if you train it for 200 epochs. The best recorded result so far is 95.55% accuracy and was reported in this paper https://arxiv.org/pdf/2206.13424v3.pdf at page 9.\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy | Test accuracy on full data\n",
    "|------|------|------|------|------|\n",
    "|   ResNet-18  | 30 | 1.0 | 0.202 | 95.55 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "-C3mHqCk_2hj"
   },
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "8Tn9pW14_2hj"
   },
   "source": [
    "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "z8g_3ZDi_2hj"
   },
   "source": [
    "## ImageNet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "PfYEhdFb_2hj"
   },
   "source": [
    "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on : https://pytorch.org/vision/stable/models.html.\n",
    "\n",
    "__Question 4 (1 points):__ Pick a model from the list above, adapt it for CIFAR10 and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenetv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenetv3 as mbn3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "i_lh4xje_2hk"
   },
   "outputs": [],
   "source": [
    "weights = mbn3.MobileNet_V3_Large_Weights.IMAGENET1K_V2\n",
    "model = mbn3.mobilenet_v3_large(weights=weights, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Sequential(nn.Linear(in_features, 10),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters() :\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.classifier[-1].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "\n",
    "X_train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "X_test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:08<00:00,  5.75it/s]\n"
     ]
    }
   ],
   "source": [
    "loss_list, test_accuracy = train(model, device, optimizer, epochs, X_train_loader, X_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute train accuracy\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in X_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "train_accuracy = accuracy_score(y_true, y_pred)\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNet v3 performances :\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy\n",
    "|------|------|------|------|\n",
    "|   ResNet-18  | 50 | 0.07 | 0.0998\n",
    "\n",
    "We probably missed something, but we didn't spent much time on this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNeXt101_32X8D_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import (ResNeXt101_32X8D_Weights,\n",
    "                               resnext101_32x8d as rsnxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "i_lh4xje_2hk"
   },
   "outputs": [],
   "source": [
    "weights = ResNeXt101_32X8D_Weights.IMAGENET1K_V2\n",
    "model = rsnxt(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_modules"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.conv1.requires_grad = False\n",
    "model.bn1.requires_grad = False\n",
    "model.relu.requires_grad = False\n",
    "model.maxpool.requires_grad = False\n",
    "model.layer1.requires_grad = False\n",
    "model.layer2.requires_grad = False\n",
    "model.layer3.requires_grad = False\n",
    "model.layer4.requires_grad = False\n",
    "model.avgpool.requires_grad = False\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(nn.Linear(in_features, 10),\n",
    "                                   nn.Softmax(dim=1))\n",
    "model.fc.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model() :\n",
    "    weights = ResNeXt101_32X8D_Weights.IMAGENET1K_V2\n",
    "    model = rsnxt(weights=weights)\n",
    "    \n",
    "    #model.conv1.requires_grad = False\n",
    "    #model.bn1.requires_grad = False\n",
    "    #model.relu.requires_grad = False\n",
    "    #model.maxpool.requires_grad = False\n",
    "    model.layer1.requires_grad = False\n",
    "    model.layer2.requires_grad = False\n",
    "    model.layer3.requires_grad = False\n",
    "    model.layer4.requires_grad = True\n",
    "    #model.avgpool.requires_grad = False\n",
    "\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Linear(in_features, 10))#, nn.Softmax(dim=1))\n",
    "    model.fc.requires_grad = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "X_train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "X_test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:01<00:20,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [00:02<00:18,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▍                                                                      | 3/20 [00:03<00:17,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 4/20 [00:04<00:17,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 5/20 [00:05<00:16,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 6/20 [00:06<00:15,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████████                                                      | 7/20 [00:07<00:14,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 8/20 [00:08<00:13,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▎                                             | 9/20 [00:09<00:12,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 10/20 [00:10<00:11,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████████                                     | 11/20 [00:12<00:10,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▏                                | 12/20 [00:13<00:08,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████████▎                            | 13/20 [00:14<00:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▍                        | 14/20 [00:15<00:06,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 15/20 [00:16<00:05,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 16/20 [00:17<00:04,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 17/20 [00:18<00:03,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 18/20 [00:19<00:02,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 19/20 [00:21<00:01,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:22<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list, test_accuracy = train(model, device, optimizer, epochs, X_train_loader, X_test_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2922"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute train accuracy\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in X_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "train_accuracy = accuracy_score(y_true, y_pred)\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNext performances :\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy\n",
    "|------|------|------|------|\n",
    "|   ResNet-18  | 20 | 0.98 | 0.2922"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "SvkuMzLs_2hk"
   },
   "source": [
    "# Incorporating *a priori*\n",
    "Geometrical *a priori* are appealing for image classification tasks, though one might have to handle several boundary effects.\n",
    "\n",
    "__Question 5 (0.5 points) :__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "TIaY60o1_2hk"
   },
   "source": [
    "Well, images have only a few pixels, and the objects of interest occupy most of them. Performing translation, rotation or scaling is likely to cut out the object of interest from the picture. \n",
    "Additionaly, when we perform translation or rotation, some pixels are replaced by empty ones (black pixels). We might want to get rid of them. If the pictures were bigger, we could crop the images to keep only the part of interest (limiting our study to a cropped picture to remove these black pixels). But in our case, it is likely that we end up with very few pixels, and this step is, again, likely to cut out the object of interest...\n",
    "On remedy to these issue could be to apply denoising algorithm on all the pictures so that we have pictures with more pixels to work on. However, it might be computationaly expensive...\n",
    "\n",
    "Additionnaly, the idea we have in mind (but didn't tried yet) it to perform domain adaptation. We would like to employ a Domain Adversarial Neural Network (that I have been using in the context of an internship). cf https://arxiv.org/pdf/1505.07818.pdf. We could consider each type of augmentation as a domain and train a DANN to identity the domain of each picture while trying to perform classification for the main task. That could allow it to build robust features and potentially, we could get better results on the main task. For example, a domain would be defined by a specific set of transformations : we apply 10° rotation + 5 pixel translation + a specific color changes. Images resulting from these transformations would share a common domain label to be learnt by the DANN is parallel of the main classification task.\n",
    "\n",
    "Remark : we anticipate to try this DANN approach in the bonnus question as the domain classification task does not require to have access to the object label. We could use data from $\\mathcal{X}$ !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "ds6e6teG_2hk"
   },
   "source": [
    "## Data augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "-Ek5wlOo_2hk"
   },
   "source": [
    "__Question 6 (4 points):__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ with them and report the accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use combinations of the following transforms :\n",
    "\n",
    "    - transforms.RandomHorizontalFlip\n",
    "    - transforms.RandomVerticalFlip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_list = [transforms.Compose([transforms.RandomHorizontalFlip(0),\n",
    "                                       transforms.RandomVerticalFlip(0)]),\n",
    "                   transforms.Compose([transforms.RandomHorizontalFlip(0),\n",
    "                                       transforms.RandomVerticalFlip(1)]),\n",
    "                   transforms.Compose([transforms.RandomHorizontalFlip(1),\n",
    "                                       transforms.RandomVerticalFlip(0)]),\n",
    "                   transforms.Compose([transforms.RandomHorizontalFlip(1),\n",
    "                                       transforms.RandomVerticalFlip(1)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_augmentation(dataset, transforms) :\n",
    "    aug_tensors = []\n",
    "    for x in dataset :\n",
    "        tensor = x[0]\n",
    "        aug_tensor = transforms(tensor)\n",
    "        aug_tensors.append(aug_tensor)\n",
    "    return aug_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "FqCjrXGk_2hk"
   },
   "outputs": [],
   "source": [
    "def augment(dataset, transforms_list) :\n",
    "    aug_tensors_list = []\n",
    "    for transforms in transforms_list :\n",
    "        aug_tensors = single_augmentation(dataset, transforms)\n",
    "        aug_tensors_list.append(aug_tensors)\n",
    "    return aug_tensors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loader(aug_tensors_list, batch_size) : \n",
    "    # Flatten\n",
    "    arr = np.array(aug_tensors_list)\n",
    "    flattened_arr = arr.flatten()\n",
    "    flattened_list = flattened_arr.tolist()\n",
    "    \n",
    "    # Create the loader object\n",
    "    custom_dataset = CustomDataset(flattened_list)\n",
    "    data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aug_train_tensor_list = augment(X_train, transforms_list)\n",
    "aug_X_train_loader = build_data_loader(aug_train_tensor_list, 10)\n",
    "\n",
    "aug_test_tensor_list = augment(X_test, transforms_list)\n",
    "aug_X_test_loader = build_data_loader(aug_test_tensor_list, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use augmented data with ResNext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "aug_X_train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "aug_X_test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:01<00:22,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [00:02<00:19,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▍                                                                      | 3/20 [00:03<00:18,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 4/20 [00:04<00:17,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 5/20 [00:05<00:16,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 6/20 [00:06<00:16,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████████                                                      | 7/20 [00:07<00:14,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 8/20 [00:09<00:13,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▎                                             | 9/20 [00:10<00:12,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 10/20 [00:11<00:11,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████████                                     | 11/20 [00:12<00:10,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▏                                | 12/20 [00:13<00:09,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████████▎                            | 13/20 [00:14<00:08,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▍                        | 14/20 [00:16<00:07,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 15/20 [00:17<00:05,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 16/20 [00:18<00:04,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 17/20 [00:19<00:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 18/20 [00:20<00:02,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 19/20 [00:21<00:01,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:22<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list, test_accuracy = train(model, device, optimizer, epochs, X_train_loader, X_test_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3409"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute train accuracy\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in X_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "train_accuracy = accuracy_score(y_true, y_pred)\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNeXt performances on augmented data :\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy\n",
    "|------|------|------|------|\n",
    "|   ResNet-18  | 20 | 0.95 | 0.3409"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "HRUA5I8N_2hk"
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "RmyiWAPJ_2hl"
   },
   "source": [
    "__Question 7 (3 points) :__ Write a short report explaining the pros and the cons of each method that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using ResNet18 as a baseline, we've tried to use MobileNet v3. This model remains small and has a \"lightweight\" architecture, and we expected it to perform better than ResNet. However, it also seems to be very sensitive to hyperparameters, which makes it difficult to tune for our specific low-ressource task. Thus, we moved to a bigger model ResNext. ResNext is an improved version of ResNet. It is known for the cardinality parameter it introduces. This parameter allows incorporating multiple paths within each block for a better feature representation. That's we expected it to be more suitable for our transfer learning task. And it did a better job than the previous approaches\n",
    "\n",
    "We also tried to apply several transforms to the pictures to perform data augmentation : ColorJitter, Rotations, Crops. But they didn't seem to improve performances. That's why we stuck to RandomHorizontalFlip and RandomVerticalFlip. These transforms are somewhat limited and a finer search for appropriate transforms parameters could have resulted in a better data augmentation. Still, we achieved improved results with the augmented set on a ResNext model.\n",
    "\n",
    "Finally, see below for explanations about the DANN (domain adversarial neural network) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "zJ-v4Nev_2hl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test accuracies of the approaches')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = [\"ResNet18\",\"MobileNet\",\"ResNext\",\"ResNext Augmented\",\"DANN\"]\n",
    "accuracies = [0.202, 0.0998, 0.2922, 0.3409,0.2292]\n",
    "plt.bar(categories, accuracies, width=0.5)\n",
    "plt.title(\"Test accuracies of the approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "sAGp7ddN_2hl"
   },
   "source": [
    "# Weak supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "jfoJ2ww5R2FG",
    "id": "wHQRLbC3_2hl"
   },
   "source": [
    "__Bonus \\[open\\] question (up to 3 points) :__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentionned above, in this last section, we try to adapt DANN (see https://arxiv.org/abs/1505.07818) to perform transfer learning from $\\mathcal{X}$. That is, we use the pictures from $\\mathcal{X}$ WITHOUT their labels. We define two domains : samples from $\\mathcal{X}$ are labelled \"1\" and samples from $\\mathcal{X}_{\\text{train}}$ are labelled 0. The architecture is the following :\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1200/1*LH3S9zaJVX8b9VJTmn7p1Q.png\">\n",
    "\n",
    "This architecture has two outputs head. The blue one is used to perform the main task (CIFAR-10 classification) and the pink is used to distinguish between $\\mathcal{X}$ and $\\mathcal{X}_{\\text{train}}$ samples. The specificity of this architecture is that the gradients from pink head get multiplied by -1 during backpropagation. As a consequence, while the pink tries to distinguish between $\\mathcal{X}$ and $\\mathcal{X}_{\\text{train}}$ samples, the the green layers are taugh to build features that prevent the pink head from learning. This adversarial learning dynamics results in building features (in the feature f layer) that are good the main task and indicriminative with respect the domain. i.e. in principle, the model can't tell the difference between samples from $\\mathcal{X}$ and $\\mathcal{X}_{\\text{train}}$, which means that it should be better at classifying new samples, from $\\mathcal{X}_{\\text{test}}$.\n",
    "\n",
    "More precisely, we train this model during 10 epochs. At each epoch, the 100 samples from $\\mathcal{X}_{\\text{train}}$ are forwarded and backpropagated in both heads. And, at each epoch, 100 samples from $\\mathcal{X}$ are fowarded and backpropagated in the domain head (as we don't know the CIFAR-10 label for these samples, we can't train the class head with them). At each epoch, the samples from $\\mathcal{X}$ are different. At the end of the training, the model has seen 10 times the 100 samples from $\\mathcal{X}_{\\text{train}}$ and it has seen $10\\times100=1000$ samples from $\\mathcal{X}$.\n",
    "\n",
    "Unfortunately, this approach yields poor results :\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   DANN  | 10 | 0.81 | 0.2292 |\n",
    "\n",
    "\n",
    "Additionaly, we've trained the model from scratch in this endeavour. What would have been more relevant is to use a pretrained model for both heads, that we would not have need to retrain.\n",
    "Another thing we could have tried is to use a different task as the domain. For example, as mentionned earlier, we could have considered that each combination of transformation on the pictures defines a domain. Then the pink head of the head would have performed multi-class classification and learn to associate the right transformation to each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN_CNN_Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DANN_CNN_Model, self).__init__()\n",
    "        self.feature = nn.Sequential()\n",
    "        self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))\n",
    "        self.feature.add_module('f_bn1', nn.BatchNorm2d(64))\n",
    "        self.feature.add_module('f_pool1', nn.MaxPool2d(2))\n",
    "        self.feature.add_module('f_relu1', nn.ReLU(True))\n",
    "        self.feature.add_module('f_conv2', nn.Conv2d(64, 50, kernel_size=5))\n",
    "        self.feature.add_module('f_bn2', nn.BatchNorm2d(50))\n",
    "        self.feature.add_module('f_drop1', nn.Dropout2d())\n",
    "        self.feature.add_module('f_pool2', nn.MaxPool2d(2))\n",
    "        self.feature.add_module('f_relu2', nn.ReLU(True))\n",
    "\n",
    "        self.class_classifier = nn.Sequential()\n",
    "        self.class_classifier.add_module('c_fc1', nn.Linear(50 * 5 * 5, 100))\n",
    "        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))\n",
    "        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n",
    "        self.class_classifier.add_module('c_drop1', nn.Dropout2d())\n",
    "        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n",
    "        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n",
    "        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n",
    "        self.class_classifier.add_module('c_fc3', nn.Linear(100, 10))\n",
    "        self.class_classifier.add_module('c_softmax', nn.Softmax())\n",
    "\n",
    "        self.domain_classifier = nn.Sequential()\n",
    "        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 5 * 5, 100))\n",
    "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n",
    "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
    "        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 2))\n",
    "        self.domain_classifier.add_module('d_softmax', nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        #input_data = input_data.expand(input_data.data.shape[0], 3, 28, 28)\n",
    "        feature = self.feature(input_data)\n",
    "        #print(feature.shape)\n",
    "        feature = feature.view(-1, 50 * 5 * 5)\n",
    "        #print(feature.shape)\n",
    "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
    "        #print(reverse_feature.shape)\n",
    "        class_output = self.class_classifier(feature)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "\n",
    "        return class_output, domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DANN (model, device, optimizer, epochs,\n",
    "                X_train_loader, aug_X_train_loader,\n",
    "                X_test_loader,\n",
    "                scheduler=None) :\n",
    "    \n",
    "    loss_class = torch.nn.NLLLoss()\n",
    "    loss_domain = torch.nn.NLLLoss()\n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "        for i,(batch,aug_batch) in enumerate(zip(X_train_loader,aug_X_train_loader)):\n",
    "            my_net.zero_grad()\n",
    "            batch_size = batch[0].shape[0]\n",
    "            aug_batch_size = aug_batch[0].shape[0]\n",
    "            len_loaders = min(batch_size,aug_batch_size)\n",
    "            p = float(i + epoch * len_loaders) / n_epoch / len_loaders\n",
    "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            \n",
    "            # CLASS SAMPLES\n",
    "            input_img = batch[0].to(device)\n",
    "            class_label = batch[1].to(device)\n",
    "            domain_label = torch.zeros_like(class_label).to(device)\n",
    "            \n",
    "            class_output, domain_output = model(input_data=input_img, alpha=alpha)\n",
    "            err_s_label = loss_class(class_output, class_label)\n",
    "            err_s_domain = loss_domain(domain_output, domain_label)\n",
    "\n",
    "            # DOMAIN SAMPLES\n",
    "            input_img = aug_batch[0].to(device)\n",
    "            domain_label = torch.ones_like(aug_batch[1]).to(device)\n",
    "\n",
    "            _, domain_output = model(input_data=input_img, alpha=alpha)\n",
    "            err_t_domain = loss_domain(domain_output, domain_label)\n",
    "            \n",
    "            # COMBINATION\n",
    "            err = err_t_domain + err_s_domain + (aug_batch_size/batch_size)*err_s_label\n",
    "            err.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # EVALUATE ON TRAIN SET\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in X_train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images, None)[0]\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        train_accuracy = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Accuracy on train set: {train_accuracy}\")\n",
    "        \n",
    "        # EVALUATE ON TEST SET\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in X_test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images,None)[0]\n",
    "                #print(outputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        test_accuracy = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Accuracy on test set: {test_accuracy}\")\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_X_train = ConcatDataset([X_train]*10)\n",
    "conc_X_train_loader = DataLoader(concatenated_X_train, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tronc = Subset(X, range(1000))\n",
    "X_tronc_loader = DataLoader(X_tronc, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net = DANN_CNN_Model()\n",
    "\n",
    "optimizer = optim.Adam(my_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:06<00:54,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.165\n",
      "Accuracy on train set: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [00:12<00:51,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.18\n",
      "Accuracy on train set: 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [00:18<00:43,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.1817\n",
      "Accuracy on train set: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:25<00:38,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2091\n",
      "Accuracy on train set: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:31<00:30,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2177\n",
      "Accuracy on train set: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:37<00:25,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2202\n",
      "Accuracy on train set: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [00:44<00:19,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2234\n",
      "Accuracy on train set: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [00:50<00:12,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2314\n",
      "Accuracy on train set: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [00:57<00:06,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2307\n",
      "Accuracy on train set: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:03<00:00,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.81, 0.2292)"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_DANN (my_net, device, optimizer, n_epoch,\n",
    "            conc_X_train_loader, X_tronc_loader,\n",
    "            X_test_loader,\n",
    "            scheduler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DANN's performances :\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy\n",
    "|------|------|------|------|\n",
    "|   ResNet-18  | 10 | 0.81 | 0.2292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "jfoJ2ww5R2FG",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
