- Compter les gradients propagés dans un simple MLP
- Compter les gradients propagés par LoRA
- Définir une métrique pour HPO
- Faire HPO
- Définir une métrique pour "cout d'un modèle"
- Définir une métrique "trade off entre cout et performance"
- Comparer retraining vs LoRA ANN vs LoRA SNN
- Y a-t-il quelque chose comme une "future adaptation de LoRA aux SNN layers"

Remarque : pour l'instant, on croit qu'il n'y a pas vraiment de SNN layer. Le modèle du neurone vient seulement remplacer la fonction d'activation et introduit de la non linéarité (il n'introduit pas de paramètre à entrainer en plus de ceux du Linear layer sous-jacent, seulement des hyper-paramètres).