\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[left=3cm, right=3cm, top=2cm, bottom=2cm]{geometry}
\usepackage[style=numeric]{biblatex}
\addbibresource{refs.bib}

\title{[MVA] - Rapport de stage}
\author{Mathis Reymond}
\date{April 2024}

\begin{document}

\maketitle

\section{Introduction}

Continual learning is a machine learning paradigm that focuses on the ability of a model to learn and adapt to new information over time, without forgetting previously learned knowledge. It is particularly relevant in scenarios where data is non-stationary or arrives in a sequential manner. By leveraging techniques such as online learning, regularization, and knowledge distillation, continual learning enables models to continuously update their knowledge and improve performance as new data becomes available.

\subsection{Initial project}
\subsubsection{Project statement}

In this context, the aim of the project was initialy stated as follows:

\begin{quote}
    \itshape
    Catastrophic forgetting is a major challenge in continual learning systems, where a neural network needs to adapt to new tasks that differ from what it has previously learned. To tackle this, experts have devised strategies to either keep the neural network's critical weights intact or use generative models to refresh the network's memory of past tasks. However, these methods aren't perfect for hardware implementation due to their additional memory demands or the need for a separate network that constantly rehearses past information.

    Meanwhile, we understand that a system's architecture significantly impacts its ability to generalize knowledge. Enter Gradmax \cite{gradmax}, an innovative algorithm that simultaneously optimizes a network's structure and weights, and smartly allocates neurons for new tasks without disrupting existing knowledge. This method is memory and computation efficient, making it suitable for hardware applications. But it does require hardware capable of dynamically adjusting its architecture to integrate new neurons as needed. Our recent work has led to a flexible hardware architecture, Mosaic \cite{mosaic}, with multiple cores that can rearrange their connections in real-time.

    The aim of this project is to explore the integration of Gradmax with our dynamic 'Mosaic' architecture. We plan to approach this by factoring hardware limitations into the optimization process. In addition, we're considering the development of a gating network within this architecture that determines the optimal timing for assigning tasks to newly added neurons. This could leverage cutting-edge techniques like Mixture of Expert models \cite{DME}.
\end{quote}

\noindent
Let's dissect this project statement to understand the main objectives and challenges it presents.\\
\\
\noindent
\textbf{Computational costs.} The project aims to address the computational costs associated with continual learning methods. Thus we expect to have a way to measure this cost, employ as figure of merit or intregrate it to the optimization process and to compare it to the cost of other methods.\\
\\
\noindent
\textbf{Continual learning approach.} Litterature categorizes continual learning approaches into three main categories: rehearsal, regularization, and architectural. However in the process of taking into account the computational costs, we will explore the architectural approach.\\
\\
\noindent
\textbf{Gradmax} Gradmax is a method designed for learning on a single task. Thus this project invites to extend it to continual learning. \\
\\
\noindent
\textbf{Mosaic} In the process of adapting Gradmax method to the continual learning framework, we will have to consider the hardware constraints of the Mosaic architecture.




\subsubsection{Major issues}

In the very beginning of the project, we faced two major issues. The first one is a consequence of Gradmax paper misleading introduction. Indeed, in the very first paragraph of the paper, the authors introduces :
\begin{quote}
    \itshape
    Searching for the best architecture for a given task is an active research area with diverse approaches.[...] Most of these approaches are costly, as they require large search spaces or large architectures to start with. In this work we consider an alternative approach.
\end{quote}

\noindent
The genuine expectation regarding the work following such an introduction is that the authors will provide an inovative method that factorizes architecture search in the optimization process of learning a task while avoiding to train numerous high-size architectures, which is computationally expensive.\\

\noindent
However, contrary to expectations, GradMax falls short at enabling neural architecture optimization. The method consists in growing a network from a root architecture to a predetermined target architecture, by adding neurons to it through training epochs. It focuses on the initialization of added neurons (the \textit{how}), neglecting the crucial questions of \textit{when} and \textit{where} to add them, which are deferred for future research. In the paper, the decisions regarding \textit{where} and \textit{when} to add neurons are predetermined and take the form of a schedule established. Consequently, it's challenging to conclude that the GradMax framework effectively enables architecture optimization for growing artificial neural networks.\\

\noindent
Additionaly, authors do not consider the computational cost of their method in the rest of the paper. This is a major issue as the project was initially designed to reduce the computational cost of continual learning methods.

\subsubsection{Additional hurdles}

As a general concept, we considere that continual learning referes to the ability of a model to learn and adapt to new information over time, without forgetting previously learned knowledge. However, this wide definition encompasses a variety of scenarios.
First, it can be the case that there is no explicit task boundary, and the model must continuously adapt to new data without any indication of when a new task begins. This is known as incremental learning. Second, the model may be presented with a sequence of tasks, each with its own training data and objectives. In this case, the model must learn each task sequentially and adapt to new tasks without forgetting the previous ones, which is known as sequential learning. Third, the model may be presented with a series of tasks that are related or have some shared structure. In this case, the model must leverage this shared structure to learn new tasks more efficiently. This is known as multitask learning.

But even after this specification, ambiguities persist. For instance when it comes to sequential learning, some frameworks assume that the model is knows when a new task arrives, whereas others do not make this assumption.

\subsection{Revised project}


\section{Attempted approaches}
\subsection{LoRA}




\nocite{*}
\printbibliography
\end{document}
