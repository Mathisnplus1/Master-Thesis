\documentclass[11pt]{article}
\setlength{\columnsep}{0.5cm}
\usepackage[margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{titling}
%========================================================
\usepackage{tabularx}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{slashbox}
\usepackage{array}
\usepackage{tikz,colortbl}
\usetikzlibrary{calc}
\usepackage{zref-savepos}

\newcounter{NoTableEntry}
\renewcommand*{\theNoTableEntry}{NTE-\the\value{NoTableEntry}}

\newcommand*{\notableentry}{%
  \multicolumn{1}{@{}c@{}|}{%
    \stepcounter{NoTableEntry}%
    \vadjust pre{\zsavepos{\theNoTableEntry t}}% top
    \vadjust{\zsavepos{\theNoTableEntry b}}% bottom
    \zsavepos{\theNoTableEntry l}% left
    \hspace{0pt plus 1filll}%
    \zsavepos{\theNoTableEntry r}% right
    \tikz[overlay]{%
      \draw[black]
        let
          \n{llx}={\zposx{\theNoTableEntry l}sp-\zposx{\theNoTableEntry r}sp},
          \n{urx}={0},
          \n{lly}={\zposy{\theNoTableEntry b}sp-\zposy{\theNoTableEntry r}sp},
          \n{ury}={\zposy{\theNoTableEntry t}sp-\zposy{\theNoTableEntry r}sp}
        in
        (\n{llx}, \n{lly}) -- (\n{urx}, \n{ury})
        (\n{llx}, \n{ury}) -- (\n{urx}, \n{lly})
      ;
    }% 
  }%
}

%========================================================
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,    % false: boxed links; true: colored links
    linkcolor=black,     % color of internal links
    citecolor=black,     % color of links to bibliography
    filecolor=black,  % color of file links
    urlcolor=black      
     % color of external links
}
%========================================================
% Customize table reference format
\renewcommand{\tablename}{Table} % Change table name to "Table"
\renewcommand{\thetable}{\arabic{table}} % Number tables as 1, 2, 3, ...
\makeatletter
\renewcommand{\p@table}{\tablename~} % Include "Table" before the table number
\makeatother
%========================================================
\usepackage[style=numeric]{biblatex}
\addbibresource{refs.bib}
%========================================================

\newcommand{\intset}[2]{\llbracket #1, #2 \rrbracket}

\title{Critique of Unconstrained Memoryless Modular Strategies in Continual Learning}
\author{Author Name}
\date{\today}

%% Pour te faire des jeunes commentaires
\setlength{\marginparwidth}{1cm} % to comment if todonotes is not used
\usepackage[textsize=tiny]{todonotes}
\newcommand{\theo}[1]{\todo[color=violet!20]{{\bf Theo} #1}}


\begin{document}





\begin{titlepage}
    \centering

    \begin{center}
        \begin{minipage}{0.4\textwidth}
            \vspace{1cm}
            \hspace{1cm}
            \includegraphics[width=0.6\textwidth]{images/logo_ENS.png}
            
            \hspace{1cm}
            \includegraphics[width=0.7\textwidth]{images/logo_MVA.png}
        \end{minipage}
        \hfill
        \begin{minipage}{0.4\textwidth}
            \vspace{-1cm}
            \includegraphics[width=0.8\textwidth]{images/logo_ETH.png}
            \vspace{0.7cm}
            \includegraphics[width=0.8\textwidth]{images/logo_INI.png}
        \end{minipage}
    \end{center}

    
    %\centering
    %\includegraphics[width=0.3\textwidth]{images/logo_ENS.png}  %\includegraphics[width=0.3\textwidth]{images/logo_ETH.png}
    
    \vspace{0cm}
    
    %\includegraphics[width=0.3\textwidth]{images/logo_MVA.png}
    %\includegraphics[width=0.3\textwidth]{images/logo_INI.png}
    
    \vfill
    
    %\HRule \\[0.4cm]
    {\huge \bfseries Artificial Neurogenesis in the Context Continual Learning \\[0.4cm]} % Replace with your thesis title
    %\HRule \\[1.5cm]
    
    \textsc{\LARGE Master Mathematics, Vision et Apprentissage}\\[1.5cm] % Replace with your degree name
    
    \textsc{\Large Mathis REYMOND}\\[0.5cm] % Replace with your name
    
    \vfill
    
    \Large
    \begin{tabbing}
        \hspace{4cm} \= \textbf{Supervisor:} \hspace{1cm} \= Prof. Melika Payvand \\ % Replace with supervisor's name
        \hspace{4cm} \= \textbf{Institution:} \> Federal Institute of Technology Zurich \\ % Replace with institution name
        \hspace{4cm} \= \textbf{Laboratory:} \> Institute of Neuroinformatics \\ % Replace with department name
    \end{tabbing}
    
    \vfill
    
    {\large April 1st 2024 - August 15th 2024}\\[2cm] % Date
    
\end{titlepage}



\twocolumn[

\begin{abstract}
    %This paper is a critique of unconstrained memoryless modular strategies in continual learning. We argue that these strategies are not well-suited for the problem of continual learning, as they certainly don't solve it but at best, solve specific benchmarks. These approaches do not deal with the core challenges of distributing inputs to modules and merging outputs from modules. More fundamentally, we show that these strategies are inherently unable to tackle these challenges and that they mainly mitigate pollution between tasks. We also discuss the management of hyperparameters in continual learning, which is often unfair and can lead to overfitting on benchmarks. Finally, we propose a naive approach that reaches state-of-the-art performance on two benchmarks, highlighting the limitations of unconstrained memoryless modular strategies.
    In this thesis, we introduce and develop a novel approach to artificial neurogenesis in the context of continual learning. It involves the generation and integration of new neural units within an artificial neural network that is being trained, enabling it to adapt and learn without forgetting previous knowledge. After stating the specific hypotheses of our framework and comparing our method to the literature, we introduce a formalism to bring to light the various ways hyperparameters optimizations can be conducted in continual learning. Additionnaly, we motivate and introduce a validation paradigm specific to continual learning that we leverage to demonstrate overfitting on benchmarks. An important part of this work focuses on a critical analysis of a whole branch of the literature, including our own approach. After putting forward the assumptions implicitly made by numerous approaches available in the literature, this critic aims to show how they lead to inherent limitations embedded in their core.
    %Starting with a critical review of Elastic Weight Consolidation method (EWC), we argue that unconstrained memoryless modular strategies do not account for all dimensions of pollution which is an inherent limitation to solve continual learning. So why does the litterature seem to show positive results? The reason lies in the introduction of hyperparameters (HPs) that create trade-offs between the tasks. Therefore, we argue that EWC should not perform any better than any other approach that makes similar trade-offs. To verify this, we present naive baselines along with a more elaborated method that only make trade-offs and show that, with the same amount of hyperparameters, they achieve similar performance to EWC. Additionally, we study how performances are impacted by the amount of hyperparameters, from $O(1)$ to $O(n^2)$. However, with all these HPs, we are essentially overfitting on the benchmarks, which we demonstrate through the introduction of validation benchmarks.
\end{abstract}
\vspace{1cm}


\tableofcontents]



\clearpage



\section{Introduction}



\subsection{Continual learning}


We, humans are able to learn a wide variety of tasks throughout our lives. We can learn to play the piano, to speak a new language, to ride a bike, to cook a new recipe, etc. We can even learn to perform several tasks at the same time, such as playing the piano and singing simultaneously. This ability to learn a sequence of tasks without forgetting the previously learned tasks is called continual learning. Continual learning is a fundamental aspect of human intelligence, and it is a key challenge in artificial intelligence. Indeed, when an artificial model using error back-propagation \cite{backprop} is assigned to learn a sequence of tasks, its final state after training on a task is perceived as an initial state to train on the next task. Without paying attention to this fact, the state will simply be overwritten during the training procedure on the new task, leading to catastrophic forgetting of the previous task. This is a major limitation of back-propagation on neural networks, and it is a key challenge in artificial intelligence. In this work, we will introduce a novel approach to continual learning that is based on the idea of dynamically growing neurons in a neural network and we will discuss the problems associated to continual learning.

\vspace{2mm}
\noindent
In a very broad sense, one can define continual learning as

\begin{quote}
    \itshape
    \centering
    the problem of learning several tasks sequentially.
\end{quote}
Although this assumption is sometimes relaxed in the literature \cite{replay_3}, the fact that tasks have to be learnt \textit{sequentially} implies that once the training procedure on a task is done, its associated training data cannot be used again. 

\vspace{2mm}
\noindent
Continual learning comes in different flavors that vary depending on the working assumptions one makes, such as whether or not the model knows when it starts training on a new task or whether or not all tasks involve the same classes, in the context of classification. Some of the most common frameworks include the followings \cite{frameworks_1_pMNIST_critic_1} \cite{EWC_7_frameworks_2}.

\vspace{2mm}
\noindent
In \textit{Task Incremental Learning}, the model is trained sequentially on a series of tasks, with each task involving a different set of data or objectives, such as sentiment analysis and then object classification within images. The model knows which task it is dealing with both during training and inference, and it often has task-specific components or outputs. The model loses access to the old data once it starts training on a new task. This framework is very general and can be applied to a wide range of problems. In \textit{Domain Incremental Learning} framework, the model faces a sequence of tasks that involve the same objective but in different domains. An example of domain incremental use case would be the classification of images of dogs and cats. Each task would require the same thing, classifying images of dogs from images of cats, but for each task, there would only be images of cats of the same breed and dogs of the same breed. A first task might involve classifying bulldogs and Chartreux cats. And a second task might involve classifying huskies and American Curl cats. During inference, the model is not informed which domain it is operating in and must generalize across these domains. Finally, \textit{Class Incremental Learning} involves learning to solve classification tasks sequentially, where new classes are added in each task. This framework excludes all form of task identifiers. The model is expected to classify data from any of the classes it has learned so far, even as new classes are introduced. This is often considered a very challenging form of continual learning, as the model must avoid catastrophic forgetting while continuously expanding its ability to recognize new classes.

\vspace{2mm}
\noindent
Literature also acknowledges other frameworks such as \textit{Lifelong Learning} \cite{review2}\cite{review3}\cite{frameworks_3_lifelong_DEN}, and other classifications of these frameworks \cite{review1}\cite{review4}.


\subsection{Approaches to continual learning}


The problem of continual learning has been heavily investigated in the past decade, leading to the development of various approaches to mitigate catastrophic forgetting. These approaches can be broadly categorized into regularization-based, replay-based and architectural.

\vspace{2mm}
\noindent
\textit{Regularization-based} approaches, which constrain the learning process through a regularization term embedded in the loss function, aim at preventing drastic changes to model parameters that are critical for previous tasks. Foundational methods to this category of approaches include Elastic Weight Consolidation (EWC) \cite{EWC_0}\cite{EWC_nuts_and_bolts}, which estimates the importance of each parameter and penalizes changes accordingly, and Synaptic Intelligence (SI) \cite{SI}, which dynamically accumulates information about parameters importance throughout training of all the tasks.

\vspace{2mm}
\noindent
Another important group of approaches is \textit{replay-based learning} \cite{replay_1}\cite{replay_2}\cite{replay_3}\cite{replay_4}\cite{replay_5_RtF}\cite{replay_6_GEM}\cite{replay_7_DGR}, which involves storing or generating data from previous tasks to revisit during the training of new tasks. This helps maintaining performance on older tasks by refreshing the model's memory of past data. Methods like Experience Replay \cite{replay_3} store a small buffer of past samples which are forwarded to the model when training on new tasks, while Generative Replay \cite{replay_2}\cite{replay_4}\cite{replay_5_RtF}\cite{replay_7_DGR} uses a external generative model to recreate as many data from previous tasks as needed. 

\vspace{2mm}
\noindent
Finally, \textit{architectural approaches} modify the model's structure to accommodate new tasks while preserving knowledge from previous ones. Methods like Progressive Neural Networks (PNN) \cite{PNN} add new sub-networks for each task, allowing to forward knowledge transfer from old tasks dedicated sub-networks, while keeping the new ones relatively isolated. Conversely, Mixture of Expert Models \cite{moe_1}\cite{moe_2} leverage a gating system which is designed to identify and distribute tasks to expert sub-networks.

\vspace{2mm}
\noindent
However, these categories do not constitute a proper partition of the literature as they are not mutually exclusive. Approaches such as Architectural and Regularization 1 (AR1) \cite{AR1} falls both in regularization-based and architectural categories. Additionally, these categories do not encompass certain methods such as adversarial approaches \cite{EWC_6_Adversarial_CL}.


\subsection{Framework specification} \label{sec:framework_specification}


Given the variety of existing frameworks and approaches, it is essential to refine and specify our own. Here we present our specific framework and approach within the context of the previously discussed literature. While the earlier subsections categorized various approaches to continual learning to provide a concise overview of the literature, we will focus on the specific assumptions that underpin our methodology, as these are crucial for laying properly the foundation of our analysis.

\vspace{2mm}
\noindent
First, we will focus on solving classification tasks involving the same set of classes. Each task differs from one another by a specific domain of the input data. We assume that the model is aware when transitioning to the training of a new task. However, during inference time, the model does not know which task each sample belongs to. Each task must be handled strictly sequentially, meaning we do not permit any use of data from previous tasks. This is conceptually closest to domain incremental learning. Additionally, we assume the total number of tasks is unknown and cannot be leveraged during the learning process.

\vspace{2mm}
\noindent
Second, as for the methods, our approach will be \textit{modular}, meaning it will break down the learning process into sub-problems and explicitly leverage dedicated modules to address these sub-problems. In other words, we will identify and manage certain components of our neural network that encapsulate high-level features, allowing us to handle them in a targeted manner. These components, or modules, may consist of independent sub-networks, sub-networks that share some parameters, or groups of neurons distributed within the entire network.

\vspace{2mm}
\noindent
On the top of the modularity of our approach, we make two additional assumptions regarding the way we will process information. First, we will work with \textit{unconstrained} architectures, meaning that we will not impose any architectural constraint on the model that could allow the processing or implicit remembering of the specificities of the past tasks. To this regard, convolutional or graph neural networks are out of our scope. We will also work with \textit{memoryless} strategies, meaning that in addition to banning the use of data from previous tasks, we will not allow to leverage a replay buffer or generative rehearsal module.

%focus on unconstrained and memoryless strategies. This means that we will not impose any constraint on the learning process that could allow the processing or implicit remembering of the specificities of the past task, and that we will not use any memory of the past tasks. Finally, we will decompose the learning process into subproblems, which will be solved independently. This will allow us to focus on the processing of the subproblems and the merging of the solutions of the subproblems to form the solution of the original problem.



\section{GroHess}



\subsection{Motivation}


\begin{figure}
    \centering
    \includegraphics[width=0.40\textwidth]{images/motivation_GroHess.png}
    \caption{Illustration of our sense of importance of weights. When the second derivative of the loss with respect to a weight is high, the weight is considered important as modifying it even a little bit would lead to a significant change in the loss. However, when the second derivative is low, modify the weight has little impact on the loss.}
    \label{fig:motivation_GroHess}
\end{figure}

To address the problem of Continual Learning, one approach is to train independent specialized modules and a gating system that learns to recognize tasks, similar to Mixture of Experts Models \cite{moe_1}\cite{moe_2}. However, the tasks to be learned often share common features that we would like to leverage, so that only the new features of a task are learned without forgetting the old ones. The emergence of such features within a neural network relies on constraints, starting with imposing small size of the model. Therefore, we would rather start training with a model of moderate size and expand it as more tasks are introduced.

\vspace{2mm}
\noindent
Such an approach presents several challenges. First, it requires to determine which important features to retain. Then, we must figure out how to retain them. Finally, we need to determine how to use them to handle new tasks. To achieve this, we introduce GroHess, a new approach to architecture growth in the context of continual learning. GroHess uses the Hessian to identify which weights are crucial for previous tasks and which can be adjusted for new tasks. Additionally, when modification of a weight that was important for a previous task is necessary, GroHess instead adds a new weight to preserve the integrity of the original one. The importance of a weight for a given is determined by the second derivative of the loss on this task with respect to the weight, once training on this task is completed. This is illustrated in Fig.~\ref{fig:motivation_GroHess}. When the second derivative of the loss with respect to a weight is high - such as in $w_1$ - the weight is considered important as modifying it even a little bit would lead to a significant change in the loss. Conversely, when the second derivative is low - such as in $w_2$ - modifying the weight has little impact on the loss. This allows us to determine which weights are important and which are not. Then, when back-propagation encourages the modification of an important weight, we refrain from updating this weight, but add a new weight by growing a new neuron and performing the update on that weight. Overall, the growing process involves selecting key neurons that are both relevant for a previous task and sensitive during learning of the current task, advocating for the need to grow a new neuron.


\subsection{Description}


\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{center}
        \includegraphics[width=0.95\textwidth]{images/GroHess_explication_1.png}
        \end{center}
        \caption{The Hessian mask is computed at the end of training on task $n-1$. In this illustration, an important weight is represented in blue. And the Gradient mask is computed during training on task $n$. In this illustration, the two weights in red have to be strongly updated according to back-propagation. Here, there is an important weight that has to be strongly updated, so it is a critical one. As a consequence, GroHess will grow a new neuron.}
        \label{fig:GroHess_explication_1}
    \end{subfigure}
    \hspace{3mm}
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{center}
        \includegraphics[width=0.95\textwidth]{images/GroHess_explication_2.png}
        \end{center}
        \caption{The critical weight is frozen by GroHess and the back-propagation update that should have been performed on the critical weight is applied to a copy of it, made through the addition of a new neuron. The critical weight will remain frozen now on, while the grown neuron will be optimized as any other neuron during the rest of the training process on task $n$.\\
         \\}
        \label{fig:GroHess_explication_2}
    \end{subfigure}
    \caption{Illustration of the GroHess procedure.}
\end{figure*}

More specifically, we control the growth of the model through two percentiles : a gradient percentile that regulates which weights are considered to have a high gradient, relatively to the other weights, and a Hessian percentile that accounts for weights for which the derivative of the loss with respect to them is high.

\vspace{2mm}
\noindent
Once the training procedure on a task is done, we compute the second derivative of the loss with respect to each weight through a random portion of the data batches of the task ($10\%$). When the second derivative of a given weight with respect to the loss falls above the hessian percentile, it means that modifying this weight will strongly disturb what has been learnt on the task. Thus, we compute a binary mask recording which weights are important for the task according to the second derivative. Conversely, when training on a latter task, if the loss starts to stagnate, we take a look at the last computed gradient with back-propagation. If the gradient of a given weight falls above the gradient percentile, it means that according to back-propagation, this weight has to be strongly updated. As a consequence, when both the first and the second derivatives of a given weight are high, we have a tension that leads us into growing a new neuron, to accommodate both back-propagation on the task at hand and the preservation of the knowledge of the previous task. The weight we grow from is then frozen; it wont be updated ever again, but we will keep back-propagating gradients through it.

\vspace{2mm}
\noindent
Every time a neuron is grown, it has to be initialized, which means that 6 choices have to be made: the value of its bias, the value of the incoming weights, the value of the outgoing weights and their respective gradients. The impact of a growth is to allow both the preservation of the knowledge of the learnt tasks and the learning of the new task. As a consequence, we mostly copy the neuron we are growing from, and we only set to zero the outgoing weights whose first and second derivatives were not both high. This choice is motivated by the fact that we do not want the grown neuron to disrupt the learning process. See Alg.~\ref{alg:GroHess}

\begin{algorithm*}
    \caption{Training GroHess on $n$ tasks}
    \begin{algorithmic}[1]
    \State \textbf{Input:} \\
    - Model $\Sigma_{ini}$ \\ 
    - List of $n$ tasks $L = [(X_1,Y_1), \ldots, (X_n,Y_n)]$\\
    - Hessian percentile $\tau_{hessian}$ \\
    - Gradient percentile $\tau_{gradient}$
    \State \textbf{Output:} Trained model $\Sigma_{fin}$ solving each task
    \State
    
    \State \textbf{Step 1:} Train $\Sigma_{ini}$ on $(X_1,Y_1)$
    \State \textbf{Step 2:} Compute $\nabla^2_{\Sigma_{ini}} \mathcal{L}_{(X_1,Y_1)}$ for each weight and define the binary mask $\mathcal{M}_{hessian}$ using $\tau_{hessian}$  
    \For{each remaining task $(X_i,Y_i)$}
        \For{each batch $(x_i,y_i)$ in $(X_i,Y_i)$}
            \If{loss stagnates}
                \State \textbf{Step 3:} Compute $\nabla_{\Sigma_{ini}} \mathcal{L}_{(x_i,y_i)}$ and define the binary mask $\mathcal{M}_{gradient}$ using $\tau_{gradient}$
                \State \textbf{Step 4:} Compute $\mathcal{M}_{overlap}$ as the element-wise product of $\mathcal{M}_{hessian}$ and $\mathcal{M}_{gradient}$
                \State \textbf{Step 5:} Grow as many neurons on $\Sigma_{ini}$ as $\mathcal{M}_{overlap}$ has columns with at least a value equal to $1$
                \State \textbf{Step 6:} Freeze the non-zero weights in $\mathcal{M}_{overlap}$
            \EndIf
            \State \textbf{Step 7:} Perform training step on $(x_i,y_i)$
        \EndFor
        \State \textbf{Step 8:} Compute $\nabla^2_{\Sigma_{ini}} \mathcal{L}_{(x_i,y_i)}$ and update $\mathcal{M}_{hessian}$ using $\tau_{hessian}$
    \EndFor
    \end{algorithmic}
    \label{alg:GroHess}
\end{algorithm*}


\subsection{Baselines}


% Doing nothing initial size
% Doing nothing mean final size
% Growing without GroHess
% Using GroHess to freeze important weights without growing

To evaluate the effectiveness of our method, which incorporates both a Hessian-inspired mechanism for determining weight importance and a growth mechanism for adding new neurons to the model, we introduce four baselines. All the baselines share the same optimizable hyperparameters and the values of these hyperparameters are optimized independently for each baseline.

\vspace{2mm}
\noindent
In a first "vanilla" baseline, we define a model in the same initial configuration as the one used to optimize with GroHess. During the training procedure through the tasks, no additional neurons are grown, and no special mechanism is used to adjust or freeze weights according to their importance for the tasks. The model remains at its original size throughout the training process. 

\vspace{2mm}
\noindent
We also train a second "vanilla" baseline, but the initial size of its hidden layers is that of the average final hidden layer sizes when training with GroHess. So, from the beginning, this baseline has the average architecture of a model trained with GroHess. 

\vspace{2mm}
\noindent
Additionally, we design a third baseline, where we perform growth without GroHess, during the training procedure. This baseline involves starting the training process on the first task with a model of the same size as the one used with GroHess. However, every time the model moves to training on a new task, it blindly adds new neurons to each layer, randomly initialized, and without freezing any weight. This baseline helps evaluate the benefits of our approach of defining weights importance through Hessian coefficients. 

\vspace{2mm}
\noindent
Finally, our fourth and last baseline uses GroHess to freeze important weights without triggering the growing mechanism. This baseline is initialized in the same way as the second one, however, we leverage GroHess to freeze important weights during the training process, but without growing new neurons as GroHess would normaly do. This baseline is supposed to isolate the effectiveness of the growing mechanism to improve perfomances.

\vspace{2mm}
\noindent
These baselines allow us to thoroughly analyze the contribution of the two main mechanisms of our approach, namely the Hessian-inspired weight importance mechanism and the growth mechanism, independently and in combination.



\section{Main experiment}



\subsection{Permuted MNIST}


\begin{figure*}
    \centering
    \includegraphics[width=0.65\textwidth]{images/p-MNIST.png}
    \caption{Illustration of the process of creating a p-MNIST benchmark. A p-MNIST dataset is a variation of the classic MNIST dataset, where pixel positions are shuffled by a fixed random permutation, altering the image's visual structure while keeping the labels intact. A p-MNIST task involves solving the classification problem for such a dataset, and a p-MNIST benchmark is created by selecting a subset of these tasks.}
    \label{fig:p-MNIST}
\end{figure*}

A permuted MNIST (p-MNIST) dataset \cite{pMNIST} is a variant of the classic MNIST dataset \cite{mnist} where images are transformed through a fixed random permutation of pixel positions, creating a shuffled version of each original image. The label associated with each image remains the same, meaning the digits themselves are unchanged, but their visual structure is modified. Then, a p-MNIST task is defined as solving the classification problem associated to a p-MNIST dataset. As each p-MNIST dataset is generated by a different permutation of MNIST dataset, there are $784!$ possible p-MNIST datasets and as many p-MNIST tasks. Finally, a p-MNIST benchmark is created by selecting a subset of these tasks. See Fig.~\ref{fig:p-MNIST} for illustration.

\vspace{2mm}
\noindent
Permuted MNIST is particularly significant in the context of continual learning as it offers the opportunity to generate arbitrary long sequences of tasks with a fixed number of classes and a fixed number of samples per class. Additionally, users have a great control over the difficulty of a benchmark, which is directly associated with the similarity of the permutations used to generate the tasks of the benchmark. Finally, the p-MNIST benchmark is a well-established benchmark in the continual learning literature, which allows for comparison of different approaches.

\vspace{2mm}
\noindent
However, literature has come with a lot of criticism about the p-MNIST benchmarks \cite{frameworks_1_pMNIST_critic_1}\cite{pMNIST_critic_2}. First, they are rather simple benchmarks, which do not reflect the complexity of real-world tasks. Second, the random permutations break the spatial structure of images we can find in the real world, which means that the p-MNIST benchmarks are not well-suited to evaluate methods that require spatial reasoning on tasks such as object detection or segmentation. In particular, the permutations of pixels disrupt the assumption of locality and translation invariance which are at the basis of convolutional neural networks. As a consequence, approaches leveraging convolutional neural networks are disadvantaged on p-MNIST benchmarks.

\vspace{2mm}
\noindent
Despite this criticism, we have chosen to use p-MNIST benchmarks. They are relevant for our study of GroHess and the critic we develop in the latter sections as their very simplicity allows us to shed light on the fundamental principles of our exploratory method and continual learning. Additionally, the ability to generate not only several tasks, but also several comparable benchmarks will allow us to motivate and introduce a validation paradigm in continual learning.


\subsection{Training protocol} \label{sec:training_protocol}


Our model consists of a multi-layer perceptron with 2 hidden layers (or 3 layers in total). Input and output sizes are 784 and 10 respectively. Each hidden layer initially contains 300 neurons. The growth of the model happens from the output layer, which means that when a weight has both a high first and second derivatives, we grow a neuron on the layer preceeding it (in the forward sense). We control the growth of the model through two percentiles : the gradient percentile $\tau_{gradient}$ and the Hessian percentile $\tau_{hessian}$. When both first and second derivative of the loss with respect to a weight are above the gradient and Hessian percentiles, respectively, we grow a new neuron. 

\vspace{2mm}
\noindent
We train our models following GroHess algorithm on a sequence of $10$ p-MNIST tasks, each with the usual $10$ MNISTS classes. For each task $80\%$ of the train set is used for training, $20\%$ is used for validation and $100\%$ of the test set is used to test the model. We train GroHess with the Adam optimizer and the cross-entropy loss. For each task, the number of epochs and the learning rate are left as optimizable hyperparameters. We use a batch size of $128$. See Tab.~\ref{table:HPs} for the complete list of hyperparameters.


\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Hyperparameters (HPs)}} \\
    \hline
    \textbf{Fixed HPs} & \textbf{Value}\\
    \hline
    Number of layers & 3 \\
    Initial hidden layers size & 300 \\
    Batch size & 128 \\
    Optimizer & Adam \\
    Loss & Cross-entropy \\
    Growth happens from & Output \\
    Gradient percentile & 0.98 \\
    Hessian percentile & 0.98 \\
    Growth trigger & 20\\
    \hline
    \hline
    \textbf{Optimized HPs} & \textbf{Range}\\
    \hline
    Learning rate & 1e-5 - 2e-3 \\
    Number of epochs & 2 - 10 \\
    \hline
    \end{tabular}
    \caption{Hyperparameters value or range}
    \label{table:HPs}
\end{table}


As the training loop is called once for each task, it means that a learning rate and number of epoch have to be found through hyperparameter optimization (HPO) for each task. HPO is performed using the Optuna library \cite{optuna} with the Tree Parzen Estimator (TPE) algorithm. We use the validation set to evaluate the performance of the model and to optimize the hyperparameters. We use the average accuracy over all tasks as the metric to optimize. We perform HPO for each task sequentially, meaning that once the best hyperparameter values for a task are found, we retrain the model with them before moving to the next task. We repeat this process $10$ times with random seeds 88 to 92 and we report the average test accuracy over all runs.



\section{Results}



\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/accs_matrix.png}
        \caption{Test accuracy on each task as GroHess move forward through the tasks.}
        \label{fig:test_accs_matrix}
    \end{subfigure}
    \hspace{-0mm}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/agg_avg_acc_curve.png}
        \caption{Average test accuracy on tasks trained so far.}
        \label{fig:classic_plot}
    \end{subfigure}
    \caption{Evolution of the average test accuracy during training. The plots show the average test accuracy over all runs on seed 88 to 92. Fig.~\ref{fig:classic_plot} also represents the standard deviation over runs.}
\end{figure*}

In Fig.~\ref{fig:test_accs_matrix}, we show the test accuracy on each task as GroHess moves forward through the tasks. This visualization provides a better insight than aggregated metrics as it allows us to inspect directly the performance of the model on each task during training. An interesting feature of our method is that the accuracies below the diagonal of the matrix, on the same line are relatively equal, which means that our GroHess is able to retain knowledge of the previous tasks without completely overwriting them. Additionally, we observe that column-wise, the accuracy is decreasing, which is reasonable: as the model learns tasks, it has to remember more past tasks, which makes the learning process harder.

\vspace{2mm}
\noindent
Additionally, Fig.~\ref{fig:classic_plot} shows the average test accuracy on tasks trained so far. In other words, the value associated to abscissa $i$ is the average of the accuracies below the diagonal of the $i^{th}$ line of the matrix in Fig.~\ref{fig:test_accs_matrix}. This plot is a more classical way to represent the performance of the model as it moves forward through the tasks, as it shows the ability of the approach to learn the first tasks of the benchmark. The last point of this plot represent the average test accuracy over all tasks, once the model has been trained on all of them.


\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.75\textwidth]{images/lit.png}
        \caption{Our results, along with results collected in the literature on the 10 tasks p-MNIST benchmark \cite{SI}\cite{LwF}\cite{replay_6_GEM}\cite{MAS}\cite{EWC_0}\cite{replay_7_DGR}\cite{HAT}\cite{EWC_6_Adversarial_CL}\cite{replay_5_RtF}.}
        \label{fig:results}
    \end{subfigure}
    \hspace{-0mm}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.75\textwidth]{images/lit_grouped.png}
        \caption{Same results as in Fig.~\ref{fig:results}, but grouped by type of approach.\\}
        \label{fig:results_grouped}
    \end{subfigure}
    \caption{Results}
\end{figure*}

\vspace{2mm}
\noindent
Additionally, we present the results obtained with GroHess along with a comparison to literature in Fig.~\ref{fig:results}. We also compare our approach to the baselines we introduced in the previous section. Each dot in this plot represent the average accuracy over all tasks of a p-MNIST benchmark after training on all of them sequentially. For both our methods and the methods reproduced, the error bars represent the standard deviation over runs on distinct seeds. The permutations used to generate the p-MNIST tasks are the same for all the methods we ran, namely the first permutation as suggested by the random module of Numpy when provided 10 different seeds.

\vspace{2mm}
\noindent
Then, we group the results by type of approach in Fig.~\ref{fig:results_grouped}. We identify detached groups. The methods relying on replay-based learning or rehearsal are performing much better than the other methods. The performances of the regularization methods are more spread, but they are generally better than our baselines and GroHess. Interpretation of the results is available through section \ref{sec:interpretation}.



\section{Paradigmatic discussion}



\subsection{Hyperparameters optimizations}


In section \ref{sec:training_protocol}, where the training protocol is introduced, we suggested to have only two hyperparameters, the number of epochs and the learning rate. However, as we mentioned, we performed one HPO for each task we train our model on. As a consequence, despite having two hyperparameters, we end up with $10$ values for each of them, which means that the whole training procedure on the benchmark carries $20$ hyperparameters. 

\vspace{2mm}
\noindent
Continual learning enables several ways to manipulate hyperparameters. In this subsection, we will discuss various approaches to manipulate them. This will help us understand that the various approaches to hyperparameter optimization (HPO) in continual learning are not equivalent: some are reasonable, others are problematic, and some violate fundamental assumptions of continual learning. Please note that the following discussion is independent of the specific optimization objective, focusing instead on the design of the HPO process itself.

\vspace{2mm}
\noindent
Let us introduce a hyperparameter $\lambda$. One could set this hyperparameter to be the same for all the tasks, which would result in a total of $1$ hyperparameter. To set the value of this hyperparameter, one could perform a single HPO on the first task, and keep using the same value for the forthcoming task. However, when proceeding that way, the hyperparameter is not optimized for each task, but only for the first one. Therefore, this way of performing HPO is not suited for our continual learning framework. Alternatively, in order to have a single value of $\lambda$ for the whole benchmark while optimizing it for all the tasks, one could perform a single HPO on the whole benchmark. It means that the HPO algorithm would suggest a value for $\lambda$ that would be used to train all the tasks. The trial would end once the model has been trained on all the tasks. At the end of such an HPO, we would end up with a single value of $\lambda$ that would be tuned for all the tasks. However, optimizing in such a way requires to have access to all the tasks at the same time, which breaks the continual learning framework. Indeed the value of $\lambda$ used when training on the first task has to be decided before accessing data from the latter tasks. Overall, this way of performing HPO is highly problematic as it causes data leakage. That is why we refer to it as a cheated HPO with respect to continual learning framework. See Tab.~\ref{tab:HPOs} top-left.

\vspace{2mm}
\noindent
Alternatively, one could set this hyperparameter to a specific value $\lambda_i$ for each task $i$, this would require to perform an HPO on each task and would result in a total of $n$ hyperparameters. This way of performing HPO is suited for continual learning framework, as tasks are treated strictly sequentially. This is how we proceeded when training with GroHess. See Tab.~\ref{tab:HPOs} bottom-center. As optimizing for the value of $\lambda_{i+1}$ requires to have the value of $\lambda_i$, we refer to this approach as greedy HPO.

\vspace{2mm}
\noindent
Remarkably, one could merge both of the previous approaches in a cheated HPO with $n$ parameters. This would require to perform a single HPO on the whole benchmark, but instead of suggesting a single value of $\lambda$ at each trial, the HPO algorithm would suggest values $\lambda_1,...,\lambda_n$, one for each task $i$. This way of performing HPO is even more problematic than the first way as it combines data leakage and a rather high number of hyperparameters. See Tab.~\ref{tab:HPOs} top-center.

\vspace{2mm}
\noindent
Finally, we will see in this paragraph how some methods in the literature can enable for HPO method that has not 1, not $n$, but $O(n^2)$ values of $\lambda$. Indeed, some methods in the literature introduce a hyperparameter $\lambda$ that is explicitly used to make trade-offs between how much the model should learn from the new task versus how much it should retain from the previous tasks \cite{EWC_0}\cite{SI}\cite{afec}\cite{EWC_nuts_and_bolts}. Such a hyperparameter can be optimized using any of the approaches described above, but it may also be used to perform trade-offs directly between tasks. For instance, when training on task 2, one could have a $\lambda_{2,1}$ that would be used to make trade-offs between how much the model should learn from task 2 and how much it should remember from task 1. When moving to task 3, one could have a $\lambda_{3,1}$, to tune how much the model should learn from task 3 and how much it should remember from task 1 along with a $\lambda_{3,2}$ for how much the model should learn from task 3 versus how much it should remember from task 2. More generally, one could even set a different value for this hyperparameter for each pair of tasks, meaning that one could introduce $\lambda_{i,j}$ for mitigating catastrophic forgetting of task $j<i$ when learning task $i>1$. Ultimately, this would result in a total of $\frac{(n-1)n}{2}$ hyperparameters. Similarly to the HPO with $n$ hyperparameters, this way of performing HPO could fit the continual learning framework, see Tab.~\ref{tab:HPOs} bottom-right, but it could also be done is a cheated way, see Tab.~\ref{tab:HPOs} top-right.

%The perspective of most papers is to introduce a hyperparameter at the task-level, generaly in the loss used to train on a task. That is, they write the loss used while training on task 1 and the loss used for training on task 2, where they introduce the hyperparameter. This hyperparameter appears as an attempt to control the trade-off between the importance of the new task and the importance of the old tasks. However, they remain fuzzy about what happen to this hyperparameter on the latter tasks and the way to choose the value for the hyperparameter. 

%hyperparameters $\it{i.e.}$ $O(n^2)$ hyperparameters. Thus, it appears that the fuzziness of authors about the precise number of hyperparameters is not a minor issue as it can lead to very different hyperparameters manipulation. 

%Not only is the number of hyperparameters introduced by the authors not clear, but also the way they are chosen. If they introduce only $1$ hyperparameter, that is the same for all tasks, then in the best case, it is chosen by hand, which is a very bad practice. In this case, the authors should at least explain why they chose this value or provide a heuristic, which is often not the case. In the worst case, they choose this hyperparameter through a hyperparameter optimization over all the tasks $-$ see [\ref{tab:HPOs}] top-left $-$ which is highly problematic due to data leakage. This breaks the continual learning framework as, after deployement, one has to decide the value of the hyperparameter when training on task $2$ without accessing data from the latter tasks to help deciding. This is considered cheating in the context of continual learning. However, it seems that several papers are doing so. The same could be performed with in the $O(n)$ and $O(n^2)$ hyperparameters scenarios $-$ see [\ref{tab:HPOs}] top-middle and top-right $-$ which would be even more problematic, but as it looks even more obviously problematic, it seems that no one had the idea to do so.

%In the three cases discussed above, we considered a single HPO performed on the training through all tasks simultaneously, which we regard as unfair. An alternative approach is to perform an HPO for each task individually, in a greedy fashion. This approach assumes at least one hyperparameter by task, so $O(1)$ hyperparameters is not suited $-$ see [\ref{tab:HPOs}] bottom-left. This approach enables defining $O(n^2)$ hyperparameters $\lambda_{i,j}$ for mitigating catastrophic forgetting of task $j<i$ when learning task $i>1$, where one performs an HPO on task $i$ once the model is already trained on tasks $j<i$, before moving to task $i+1$ $-$ see [\ref{tab:HPOs}] bottom-right. However, we argue that it represents to many parameters, and makes questionnable the sacalability of this approach. Additionnaly, it is likely to lead to an overfitting on benchmark through over-specificity of trade-offs. Finally the most reasonable approach seems to perform an HPO on each task individually, in a greedy fashion, but with only $O(n)$ hyperparameters, one for each task $-$ see [\ref{tab:HPOs}] bottom-middle. This approach is compatible with the continual learning framework, as setting the hyperparameter for training on task $i$ can be done without accessing data from the latter tasks.

\begin{table*}[h!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|c|}
        \hline
        \diagbox{Number of HPOs}{Number of HPs} & $O(1)$ & $O(n)$ & $O(n^2)$ \\ \hline
        $1$ cheated HPO
        & 
        \centering
        \begin{tabular}{cc}
            \begin{tabular}{|ccc|}
                \hline
                $T_1$ & $\cdots$ & $T_n$ \\ \hline
                \multicolumn{3}{|c|}{$\{\lambda\}$} \\ \hline
            \end{tabular}
            &
            \hspace{-5mm}
            \begin{tabular}{c}
                                                        \\
                                                        \\
                                                        \\
                \hspace{-1.5mm}\footnotesize{$HPO$}\\
            \end{tabular}
        \end{tabular}
        & 
        \centering
        \begin{tabular}{cc}
            \begin{tabular}{|ccc|}
                \hline
                $T_1$ & $\cdots$ & $T_n$ \\ \hline
                \multicolumn{3}{|c|}{$\{\lambda_1,...,\lambda_n\}$} \\\hline
            \end{tabular}
            &
            \hspace{-5mm}
            \begin{tabular}{c}
                                                        \\
                                                        \\
                                                        \\
                \hspace{-1.5mm}\footnotesize{$HPO$}\\
            \end{tabular}
        \end{tabular}
        &
        \begin{tabular}{cc}
            \begin{tabular}{|ccc|}
                \hline
                $T_1$ & $\cdots$ & $T_n$ \\ \hline
                \multicolumn{3}{|c|}{$\left\{\lambda_{i,j}|\forall 2\leq j<i\leq n\right\}$} \\\hline
            \end{tabular}
            &
            \hspace{-5mm}
            \begin{tabular}{c}
                                                        \\
                                                        \\
                                                        \\
                \hspace{-1.5mm}\footnotesize{$HPO$}\\
            \end{tabular}
        \end{tabular}
        \\ \hline
        $n$ greedy HPOs
        & 
        \notableentry 
        &
        \centering
        \begin{tabular}{cccccc}
        %    \begin{tabular}{|c|}
        %        \hline
        %        $T_1$    \\\hline
        %        $\emptyset$  \\\hline
        %    \end{tabular}
        %    &
        %    \hspace{-6mm}
        %    \begin{tabular}{c}
        %                                            \\
        %        \hspace{2mm} $\longrightarrow$      \\
        %                                            \\
        %                                            \\
        %    \end{tabular}
        %    &
        %    \hspace{-4mm}
            \begin{tabular}{|c|}
                \hline
                $T_1$    \\\hline
                $\{\lambda_1\}$  \\\hline
            \end{tabular}
            &
            \hspace{-6mm}
            \begin{tabular}{c}
                                                                            \\
                \hspace{2mm} $\longrightarrow$ \ldots $\longrightarrow$       \\
                                                                            \\
                \hspace{-14mm}\footnotesize{$HPO_1$}                          \\
            \end{tabular}
            &
            \hspace{-4mm}
            \begin{tabular}{|c|}
                \hline
                $T_n$    \\\hline
                $\{\lambda_n\}$  \\\hline
            \end{tabular}
            &
            \hspace{-6mm}
            \begin{tabular}{c}
                                                    \\
                                                    \\
                                                    \\
                \hspace{-1.5mm}\footnotesize{$HPO_n$}\\
            \end{tabular}
        \end{tabular}
        & 
        \begin{tabular}{cccccc}
        %    \begin{tabular}{|c|}
        %        \hline
        %        $T_1$    \\\hline
        %        $\emptyset$  \\\hline
        %    \end{tabular}
        %    &
        %    \hspace{-6mm}
        %    \begin{tabular}{c}
        %                                            \\
        %        \hspace{2mm} $\longrightarrow$      \\
        %                                            \\
        %                                            \\
        %    \end{tabular}
        %    &
        %    \hspace{-4mm}
            \begin{tabular}{|c|}
                \hline
                $T_2$    \\ \hline
                $\{\lambda_{2,1}\}$ \\ \hline
            \end{tabular}
            &
            \hspace{-6mm}
            \begin{tabular}{c}
                                                    \\
                \hspace{2mm} $\longrightarrow$      \\
                                                    \\
                \hspace{-3mm}\footnotesize{$HPO_2$} \\
            \end{tabular}
            &
            \hspace{-4mm}
            \begin{tabular}{|c|}
                \hline
                $T_3$    \\ \hline
                $\{\lambda_{3,1},\lambda_{3,2}\}$ \\ \hline
            \end{tabular}
            &
            \hspace{-6mm}
            \begin{tabular}{c}
                                                                              \\
                \hspace{2mm} $\longrightarrow$ \ldots $\longrightarrow$       \\
                                                                              \\
                \hspace{-14mm}\footnotesize{$HPO_3$}                          \\
            \end{tabular}
            &
            \hspace{-4mm}
            \begin{tabular}{|c|}
                \hline
                $T_n$    \\\hline
                $\{\lambda_{n,1},\ldots,\lambda_{n,n-1}\}$  \\\hline
            \end{tabular}
            &
            \hspace{-6mm}
            \begin{tabular}{c}
                                                     \\
                                                     \\
                                                     \\
                \hspace{-1.5mm}\footnotesize{$HPO_n$}\\
            \end{tabular}
        \end{tabular}
        \\ \hline
    \end{tabular}}
    \caption{Various ways to perform HPOs in continual learning. For a given hyperparameter $\lambda$ and tasks $T_1,...,T_n$, one can end-up with $O(1)$ (left column), $O(n)$ (middle column) or even $O(n^2)$ (right column) values of $\lambda$ depending on the design of the HPO process. Additionally, one can perform a single HPO over the whole benchmark but it breaks continual learning assumptions (first line), or one can perform an HPO for each task (second line).}
    \label{tab:HPOs}
\end{table*}

\vspace{2mm}
\noindent
Despite the choice and optimization of hyperparameters being critical, it seems that some confusion remain in the literature. The overwhelming majority of papers do not report their way of performing HPO or remain fuzzy about the management and values of certain hyperparameters \cite{EWC_0}\cite{EWC_2_measuring_cf}\cite{afec}. On the other hand, among the papers that do report their HPO procedure, some fall into the cheated HPO category \cite{EWC_3}. The end of this subsection will be dedicated to analyzing how HPO was likely conducted in a foundational paper in the literature that does not fully report the HPO procedure: the original Elastic Weight Consolidation (EWC) paper \cite{EWC_0}.

\vspace{2mm}
\noindent
EWC is a regularisation method that attempts to tackle catastrophic forgetting by penalizing changes in important weights. The weights that are important for a task are the ones that updating would cause the model to move away from the solution of the task. Authors leverage the Fisher Information Matrix to define this importance and enforce the update of important weights to be performed in a direction that does not hurt the knowledge on the tasks already learnt. As introduced in the original paper, training on task $2$ the loss used by EWC can be written as $\mathcal{L} = \mathcal{L}_{2} + \lambda\mathcal{L}_{reg}$, where $\mathcal{L}_{2}$ accounts for maximizing the performance on task 2, $\lambda$ is a positive scalar and $\mathcal{L}_{reg}$ accounts for the regularization between task 1 and 2.

\vspace{2mm}
\noindent
However, in this paper, authors do not explain how this regularization term scales with the number of tasks. It could be the case that the same value of $\lambda$ is used for all the tasks, which would lead to write the loss used while training on task $m$ as 
\begin{equation}
    \mathcal{L} = \mathcal{L}_{m} + \lambda{L}_{reg,m}
\end{equation}
or it could be the case that a different value of $\lambda$ is used for each task, which would lead to write the loss used while training on task $m$ as
\begin{equation}
    \mathcal{L} = \mathcal{L}_{m} + \lambda_{m}{L}_{reg,m}
\end{equation}
or the authors might want to make trade-offs between each pair of tasks, introducing a loss of the following form when training on task $m$
\begin{equation}
    \mathcal{L} = \mathcal{L}_{m} + \sum_{i=1}^{m-1}\lambda_{m,i}\mathcal{L}_{reg,m,i}
\end{equation}

\vspace{2mm}
\noindent
Each of these $3$ possible ways of proceeding would result in a total of $O(1)$, $O(n)$ or $O(n^2)$ hyperparameters, respectively. However, the authors do not provide any information about the way they proceeded, which is highly problematic. Additionally, authors do not report the value of the hyperparameter they introduced nor the process of optimizing it and they do not provide their code. This lack of clarity is a major issue as it makes it impossible to reproduce the results of the paper. Despite the fact that the community re-implemented the method, the method itself does not include the manipulation of hyperparameters made by authors. As a consequence, attempts to reproduce these methods are very disparate and yield contrasted results. See Fig.~\ref{fig:EWCs} for a summary of the results we identified in the literature.

\begin{figure}
    \centering
    \includegraphics[width=0.20\textwidth]{images/EWCs.png}
    \caption{Summary of the results we identified in the literature with EWC on 10 tasks p-MNIST benchmarks \cite{EWC_0}\cite{EWC_1}\cite{EWC_2_measuring_cf}\cite{EWC_3}\cite{EWC_4_ref_romain}\cite{EWC_5}\cite{EWC_6_Adversarial_CL}\cite{EWC_7_frameworks_2}}
    \label{fig:EWCs}
\end{figure}

\vspace{2mm}
\noindent
Remarkably, all the papers reporting results above or equal to $90\%$ accuracy on the 10 tasks p-MNIST benchmark are either performing a cheated HPO or not explaining their HPO procedure. In particular, the original paper mentions that the size of the hidden layers is an optimizable hyperparameter. However, the size of the model remains fixed when training on the different tasks of the benchmark. This suggests that the HPO performed by the authors of the original paper covers the entire benchmark, that is, the HPO is likely to be cheated and include $O(n)$ hyperparameters. See Tab.~\ref{tab:HPOs} top-center.


\subsection{Introduction of validation benchmarks}


Actually, what we refer to as "cheated HPO" where a single HPO is performed through all the tasks at the same time, could be just fine if one was performing the HPO on a benchmark, and then showing the performances of the approach on another benchmark with the same hyperparameters. Such an approach would not even break the hypothesis according to which we do not know the number of tasks in advance. On would just have to perform the HPO on a benchmark with a huge number of tasks, and then show the performances of the approach on another benchmark, with less tasks, using only the hyperparameters computed for the first tasks of the HPO benchmark.

\vspace{2mm}
\noindent
To better analyse methods in continual learning, we introduce a validation paradigm on benchmarks to measure overfitting on benchmarks. We perform HPO on a benchmark and then measure the performance on another benchmark on which we train the model using values of the HPs provided by the HPO performed on the HPO benchmark. However, one could argue that performing HPO on a benchmark like p-MNIST and validating the approach using optimized hyperparameters on a benchmark derivated from CIFAR, for instance, would not be fair. And in order to avoid defining a brittle and handcrafted metric complexity or similarity between benchmarks, we will stick to p-MNIST benchmarks. Indeed, this validation paradigm we are introducing is one of the reasons why we chose to use p-MNIST benchmarks in the first place. Just as we did in Fig.~\ref{fig:p-MNIST}, we generate $10$ additional p-MNIST benchmarks, each with $10$ tasks. As a consequence, we have benchmarks that share the same complexity and similarity from a statistical point of view. We do not perform any HPO on these validation benchmarks. Instead, we reuse the hyperparameter found on the previous benchmark, and simply retrain GroHess with them on the validation benchmarks. See Fig.~\ref{fig:val_accs_matrix} and Fig.~\ref{fig:accuracy_through_benchmarks} for the results of this validation paradigm.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{images/val_accs_matrix.png}
    \caption{Test accuracy on each task once training with GroHess is complete on all the tasks, for the HPO benchmark and 10 validation benchmarks.}
    \label{fig:val_accs_matrix}
\end{figure}

\vspace{2mm}
\noindent
For the HPO benchmark and each of the 10 validation benchmarks, Fig.~\ref{fig:val_accs_matrix} shows the test accuracy on each task once training with GroHess is complete on all the tasks. These results are run and averaged over seeds 88 to 92. At first glance, it seems that the performance on old tasks are better for the HPO benchmark, suggesting that it is harder for the model to mitigate catastrophic forgetting if the hyperparameters have not been specifically tuned on the benchmark. To confirm this, for each task, we compute the average test accuracy through the validation benchmarks (column wise average) and report the results in Fig.~\ref{fig:accuracy_through_benchmarks}. In addition, this figure shows the standard deviation for performances on the validation benchmarks, along with the performances obtained on the HPO benchmark (left plot). The right plot shows the same results, averaged out over all tasks. We observe that the average test accuracy over all the tasks of the HPO benchmark are greater by an amount up to $9\%$ than the averge one on the validation benchmarks. This figure confirms that, despite the fact all the benchmarks have the same complexity, performing an HPO on a benchmark is necessary to get the best performances. In other words, GroHess lack of versatility and is overfitting on the benchmark through the HPO process.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{images/accuracy_through_benchmarks.png}
    \caption{Results from Fig.~\ref{fig:val_accs_matrix} averaged over the validation benchmarks (left plot) and additionally averaged over all tasks (right plot).}
    \label{fig:accuracy_through_benchmarks}
\end{figure}

\vspace{1mm}
\noindent
These results show that the choice of hyperparameters is at the very core of the performances of our method. It means that catastrophic forgetting is mitigated not only through the network growth but also to a significant part through very specific arbitrations of how much the model should learn from new tasks as they come, that is, it seems that our method relies on trade-offs.



\section{Critical comparison to literature} \label{sec:interpretation}



In the following section, we will develop remarks about the methods fitting with our assumptions: modularity, absence of memory and absence of contraints. See subsection \ref{sec:framework_specification} for recall of the assumptions. This section aims at investigating inherent limitations of GroHess and, more broadly, modular memoryless and unconstrained methods. 


\subsection{Trade-offs in continual learning}


In the last section, we acknowledged that the hyperparameter optimization performed in the context of continual learning yields numerous hyperparameters. This can be an implicit way to perform trade-offs between how much the model should learn from the new task versus how much it should remember from the previous tasks. This lead has been confirmed through validation experiments, where we have seen that the very subtle differences between p-MNIST benchmarks are enough to impact the performances by an amount of $9\%$. As a consequence, we hypothesize that the HPO procedure is performing trade-offs between the tasks.

\vspace{2mm}
\noindent
An interesting way to investigate this hypothesis is to have a look at the specific hyperparameters retained by the HPO. Fig.~\ref{fig:best_params} shows the best values of the hyperparameters found by the HPOs ran on each task of the HPO benchmark. We observe that the learning rate is decreasing by over an order of magnitude as the model moves forward through the tasks. Similarly, we note that the best number of epochs used to train on a task was the highest allowed for the first tasks, and then quickly decreased for the latter tasks. As a consequence, the model is learning less and less of each tasks. Now we interpret this behavior as an attempt to keep track of previously acquired knowledge while learning on the new task. That is, our approach is mitigating catastrophic forgetting through trade-offs between the specificity of the tasks. As the differences between the specificity of the tasks rely on the permutations used to generate the tasks, our interpretation provides an explanation of why the model is overfitting on the HPO benchmark. Indeed, the trade-offs necessary to perform well on a given validation benchmark would require other hyperparameters than the ones found on the HPO benchmark.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/best_params.png}
    \caption{Best values of the hyperparameters (learning rate on the left and number of epochs on the right) found by the HPOs ran on each task of the HPO benchmark.}
    \label{fig:best_params}
\end{figure}

\vspace{2mm}
\noindent
Furthermore, in addition to making trade-offs between learning and memory for each pair of tasks, methods like GroHess and those based on regularization, such as EWC, SI, LwF or AFEC \cite{EWC_0}\cite{SI}\cite{LwF}\cite{afec}, incorporate trade-offs within the optimization process. Indeed, the regularization term used in these methods is intended to perform trade-offs between the importance of the new task and the importance of the old tasks. They are controlled by a scalar, which is a hyperparameter is optimized through the HPO procedure. Similarly, through the computation of the Hessian and Gradient masks of importance, GroHess is trading off how much to update the weights that are the most crucial for the past tasks versus how much to update these weights for the benefit of the new task. 

\vspace{2mm}
\noindent
Even though all these methods are performing trade-offs, all the trade-offs are not the same. From one method to another, the hyperparameters are not the same, which means that the trade-offs that can be performed through HPO are not the same. This explains the disparities between the results of regularization methods gathered in Fig.~\ref{fig:results_grouped}. Additionally, the trade-offs performed by the regularization methods are continuous, in the sense that they affect all the weights of the model more or less intensively. On the other hand, GroHess performs discrete trade-offs, only impacting a few weights, leaving the other ones completely unchanged. Finally, to explain results obtained with GroHess that happen to be even poorer than the ones yielded by our baselines, we note that our growing procedure is not optimal. Indeed, it is not embedded in an optimization procedure, as it is the case for the baselines and regularization methods who are optimized through back-propagation only. Thus, it is likely that our growing procedure is simply disturbing the learning process done through back-propagation.


\subsection{Geometrical argument}


In this subsection, we will discuss the geometrical motivation behind several regularization methods such as EWC \cite{EWC_nuts_and_bolts}\cite{EWC_0}. Very often, the method is motivated by the representation of a 2-dimensional parameter space, where two ellipses represent regions of low loss when provided data from task A or task B. See Fig.~\ref{fig:ellipses_EWC}. Once training on task A is done, one assumes that the model is in a state close to the argument of minimum of the loss for data associated to task A, $\theta_A*$. Then, the intuition is that the regularization term introduced in the method will prevent the parameters from converging directly toward the argument of the minimum of the loss for task 2, $\theta_B*$, when learning on task B. Instead, the parameters should converge to an overlapping region where both loss on task A and loss on task B are small. However, this representation is misleading, does not reflect the complexity of the problem and, as a consequence, does not hold in practice. 

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{images/ellipse_overlap.png}
    \caption{Representation of the parameters space as depicted in some regularization papers \cite{EWC_0}\cite{EWC_nuts_and_bolts}. The blue and red ellipses represent a region with low loss on task $A$ and task $B$, respectively.}
    \label{fig:ellipses_EWC}
\end{figure}

\vspace{2mm}
\noindent
A first objection is that with a regularization method, the direction of parameters modifications does not aim for a minimum of the model trained on the combined data from tasks A and B, but rather towards the local minimum of the model on task B's data that is closest to the local minimum reached by the model trained solely on task A's data.

\vspace{2mm}
\noindent
Second, the Fisher Information Matrix introduced in EWC is leveraged to approximate a metric that indicates what is considered to be the best path to move toward low loss on task $B$, compromising with increase of loss on task A. However, loss on task A is not directly leveraged, as when training on task B, samples from task A are not accessible anymore. As a consequence, authors says that in order to stay close to the region of low loss on task A, it is reasonable to try to update the parameters in a direction of high curvature of the ellipse associated to task A. Indeed, the part of the ellipse that has the highest curvature is also the one that is the furthest from $\theta_A*$, which makes it a direction in which updates of the parameters can be carried without increasing the loss on task A too much. However, contrary to the representation in Fig.~\ref{fig:ellipses_EWC}, this is not a guarantee that the model will converge to the center of the overlapping region. Indeed, the overlapping region might not be aligned with the axis passing through $\theta_A*$ and the point of ellipse associated with task A with the highest curvature. See Fig.~\ref{fig:other_ellipses}. In these representation, the red arrow represent the update direction according to $\mathcal{L}_{B}$ and the blue arrow represent the update direction enforced by $\mathcal{L}_{reg}$. The actual update direction is then decided by the value of $\lambda$ and falls inside the cone generated by the two arrows, without covering the whole cone as the linear combination is done through a single scalar. Thus, what matters in the situations depicted in Fig.~\ref{fig:ellipses_EWC} is the angle between the two arrows along with the orientation of the blue arrow relatively to the red ellipse. The orientation of the red ellipse relatively to the blue arrow accounts for the success of the update: is the update going to land in a region where both loss on task A and on task B is low? On the other hand, the angle between the red and the blue arrows accounts for how much $\mathcal{L}_{B}$ and $\mathcal{L}_{reg}$ are acting jointly. In the case depicted in Fig.~\ref{fig:two_ellipses_45}, no matter what the value of $\lambda$ is, the update will be perfomed toward the overlapping region. This means that the configuration depicted in EWC papers is a really convenient scenario. On the other hand, looking at the configuration in Fig.~\ref{fig:two_ellipses_90}, we see that the update will be performed toward the overlapping region only if the value of $\lambda$ is low enough, because $\mathcal{L}_{reg}$ is pulling the parameters away from the overlapping region. Ultimately, Fig.~\ref{fig:two_ellipses_180} depicts an extreme scenario, where $\mathcal{L}_{reg}$ is actively acting against $\mathcal{L}_B$. In this configuration, any none zero value of $\lambda$ would make the update worse both with respect to task A and B.

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{images/ellipse_overlap_45.png}
        \caption{Configuration depicted in EWC papers\\
         \\}
        \label{fig:two_ellipses_45}
    \end{subfigure}
    \hspace{-0mm}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{images/ellipse_overlap_90.png}
        \caption{A configuration less favorable where both the angle and the orientation of the red ellipse are not convenient}
        \label{fig:two_ellipses_90}
    \end{subfigure}
    \hspace{-0mm}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{images/ellipse_overlap_180.png}
        \caption{Extreme configuration where $\mathcal{L}_{reg}$ is actively acting against $\mathcal{L}_B$ \\
         \\}
        \label{fig:two_ellipses_180}
    \end{subfigure}
    \caption{Several possible configurations in the parameters space. The blue and red ellipses represent a region with low loss on task $A$ and task $B$, respectively. The red arrow represents the direction in which the loss on task $B$ is pulling the model, while the blue arrow shows the direction in which the regularization term is pulling the model.}
    \label{fig:other_ellipses}
\end{figure*}


\vspace{2mm}
\noindent
Furthermore, these illustrations represent a 2-dimensional parameters space, however, in the experiments conducted in the literature, the parameter space is $\mathbb{R}^n$ with $n \sim 10^5$ or even orders of magnitudes bigger. Additionally, as we already mentioned earlier, the number of tasks in a continual learning benchmark is generally above or equal to $10$, and the research community ultimately expects to be able to tackle as many as hundreds of tasks. As a consequence of these two remarks, the area (or volume) of the overlapping region tends doubly towards 0. On one hand, this representation accounts for two tasks, but in real life scenarios, there are many more. And as the number of tasks increases, the area of the overlapping region decreases, has 0 zero as a lower bound, so converges. But it has no reason to converge toward a non-zero value as the relative position of the low loss ellipses is \textit{a priori} independent between the tasks, or at least, nobody has provided a better lower bound than 0, to the best of our knowledge. On the other hand, the previous remark implies that there exist a number of tasks for which the overlapping region of interest is small enough to fit within an Euclidean sphere whose radius is strictly smaller than 1, which volume, therefore, converges toward 0 as the dimension of the parameters space increases. 

\vspace{2mm}
\noindent
So, an increase in the number of tasks makes it less likely to find a region where the loss is low for all the tasks, while an increase in the number of parameters of the model somewhat pulls the low loss regions apart from each other. These two remarks do not constitute a strict argument against the regularization methods but rather bring to light the fact the motivation behind them is somewhat flawed. Additionally, the explain why literature report very poor performances of EWC on longer and harder benchmarks than p-MNIST \cite{EWC_4_ref_romain}\cite{EWC_6_Adversarial_CL}. Overall, our remarks mostly point out the absence of guarantees. The absence of guaranty is not dramatic though, as deep learning tends to be able to perform well in practice without strong theoretical results. But what is more problematic is that authors of modular, memoryless and unconstrained methods generally do not even have control over the object of our remarks, such as the relative position of the ellipses.

\vspace{2mm}
\noindent
Finally, the worse inherent limitations of modular, memoryless and unconstrained methods is beautifully captured on the Fig.~\ref{fig:ellipses_EWC} :
\begin{quote}
    \itshape
    \centering
    Improving on task B means deteriorating on task A, and reciprocally.
\end{quote}
\vspace{2mm}
\noindent
It does not mean that the more tasks there are to solve, the harder it gets to solve them all, which is true. Our statement is stronger, it means that tasks are inherently put in a competition setting with each other. This implies that performing continual learning with modular, memoryless and unconstrained methods inherently boils down to perform trade-offs, which confirms our prior analysis of trade-offs through HPOs. Ultimately, it implies that modular, memoryless and unconstrained methods are inherently limited when it comes to perform continual learning on numerous tasks. By contrast, we do not find this inherent limitation with in Mixture of Experts models \cite{moe_1}\cite{moe_2}, where modules dedicated to each tasks are in parallel, and improving one particular module does not hurt another module as long as the gating system remains the same. And this gating system can be implemented using replay or rehearsal methods, or leveraging feature extractor such as CNN masks. But here we break one of our two assumptions: either we leverage some form of memory or we employ architectural constraints.


\subsection{Combinatorial argument}


In this subsection, we will provide a combinatorial point of view on the inherent limitations of modular, memoryless and unconstrained methods.

\begin{figure}
    \centering
    \includegraphics[width=0.35\textwidth]{images/PNN.png}
    \caption{Extracted from \cite{PNN}. Depiction of a three column progressive network. The first two columns on the left (dashed arrows) were trained on task 1 and 2 respectively. The grey box labelled a represent the adapter layers (see text). A third column is added for the final task having access to all previously learned features}
    \label{fig:PNN}
\end{figure}

\vspace{2mm}
\noindent
Modular approaches are based on the idea that the model can be decomposed into several modules, each of them being dedicated to a task. Working with such methods in the context of continual learning raises three questions: how to distribute a given sample to the right module? How to specialize the modules? How to merge the outputs of the modules? 
The first question could be answered by a gating system, which, however, does not respect our memoryless and unconstrained assumptions. The second question is well studied and several satisfying answers are provided in the literature. Modules can be specialized through regularization or may even be trained separately from one another. The third question is generally answered by a form of weighted linear combination of the outputs of the modules, where the weights of the linear combination are optimized through the HPO procedure. And in the answer to this last question lies the inherent limitation of modular, memoryless and unconstrained that we already put forward in the previous section: improving on a task (i.e. putting a higher weight when merging its associated module) means deteriorating on another one, and reciprocally. 

\vspace{2mm}
\noindent
An example of method that does not circumvent this limitation is called Progressive Neural Networks (PNN) \cite{PNN}. This method falls in the category of architectural approaches to continual learning. In PNN, the model is being grown every time a new task is tackled. The grown part consists of a whole deep neural network. The distribution of task is done through adapter layers which are trained to forward a sample to the next layer only if belongs to the right task. See Fig.~\ref{fig:PNN}.

\vspace{2mm}
\noindent
Each column is trained on the data of the task it is intended to solve. It also does take into account the learning of the previous tasks. However, a given column is unable to deal with samples of the latter tasks. As a consequence, a PNN may be able to solve each task, through one of the columns, once the training on all the tasks is completed. However, it remains unable to choose properly which column for the inference of a sample from a given tasks. 


\begin{table}
    \begin{tabular}{|c|c|c|}
        \hline
        \diagbox{{\footnotesize Tested}}{{\footnotesize Trained}} & Task 1 & Task 2 \\ \hline
        Task 1 & \includegraphics[width=2.5cm]{images/pollution_11.png} & \includegraphics[width=2.5cm]{images/pollution_12.png} \\ \hline
        Task 2 & \notableentry & \includegraphics[width=2.5cm]{images/pollution_21.png} \\ \hline
    \end{tabular}
    \caption{After training on task 1, there is no pollution (top left). After training on task $2$, there is $P^{1\rightarrow 2}$, pollution from task-2-specific weights when forwarding task 1 samples (top right) and $P^{2\rightarrow 1}$, pollution from task-1-specific weights when forwarding task $2$ samples (bottom right).}
    \label{table:pollution}
\end{table}

\vspace{2mm}
\noindent
The problem faced by PNN is very general, we refer to it by the term of \textit{pollution}, a concept that we will attempt to formalize below.

\vspace{2mm}
\noindent
Let us assume we are given $I$ tasks indexed in increasing order by the set $\intset{1}{I}$. We define a neural network as the set of its parameters
\begin{align}
    \phi = \left\{\left\{ w_{l,m,n} \in \mathbb{R} | m,n \in \llbracket 1, N_{l-1} \rrbracket \times \llbracket 1, N_{l} \rrbracket \right\} | l \in \llbracket 1, L+1 \rrbracket \right\}
\end{align}
where $L$ is the number of hidden layers and $N_l$ is the number of neurons on layer $l$.

\vspace{2mm}
\noindent
Let us define $\mathbb{W}_{l,n}^i$ to be the set of protected weights for task $i$ with respect to neuron $n$ of layer $l$, for any $i \in \intset{1}{I}$, $l \in \intset{1}{L+1}$ and $n \in \intset{1}{N_l}$:
\begin{align}
    \mathbb{W}_{l,n}^i = \left\{ w_{l,m,n}, m \in \intset{1}{N_{l-1}} | w_{l,m,n} \text{ protected} \right\}
\end{align}

\vspace{2mm}
\noindent
which allows to define the pollution of task $i$ by protected weights for task $j$ on neuron $n$ of layer $l$ as
\begin{align}
    p_{l,n}^{i\rightarrow j} = \mathbb{E}_{x \sim D_i}\left[\frac{\sum_{w \in \mathbb{W}_{l,n}^j}{w(x)}}{\sum_{w \in \mathbb{W}_{l,n}^i}{w(x)}}\right]
\end{align}

\vspace{2mm}
\noindent
Finally, we can define the pollution of task $i$ by protected weights for task $j$ on the whole network as
\begin{align}
    P^{i\rightarrow j} = \frac{1}{L+1}\sum_{l=1}^{L+1}{\frac{1}{N_l}\sum_{n=1}^{N_l}{p_{l,n}^{i\rightarrow j}}}
\end{align}

\vspace{2mm}
\noindent
The notion of pollution $P^{i\rightarrow j}$ encapsulates how much neurons specialized for a task $j$ interfere with neurons specialized for a task $i$ when provided a sample from task $i$, after training on the first $\max(i,j)$ tasks at least. See Tab.~\ref{table:pollution} for a simple illustration. Once the model is trained on task 1, we can test it on data from task 1 (top left cell) but we can't expect it to perform well on data from the next tasks (bottom left cell), so no pollution is occurring. Then, suppose the upper blue line to represent a module of a neuron that we want to dedicate to task 1. This blue line is the module specialized to task 1, we won't update it anymore. Once the model is trained on task 2, we expect it to perform well on task 1 and 2, so the pollutions that can occur are $P^{1\rightarrow 2}$ (top right cell) and $P^{2\rightarrow 1}$ (bottom right cell). $P^{2\rightarrow 1}$ is already problematic, but is generally handled through trade-offs as we mentioned earlier. When the model is being trained on task 2, it learns how to deal with the outputs of the module specialized for task 1 when given samples from task 2. $P^{2\rightarrow 1}$ is the pollution that can be tackled through trade-offs. However, when training on task 2 is done, the lower red module in the top right cell has been overwritten by optimization on samples of task 2 only. As consequence when provided a samples from task 1 again, during test time, the upper dedicated blue module might output reasonably well, however, the lower module, now specialized on task 2, does not know how to deal with samples from task 1. That is why $P^{1\rightarrow 2}$ cannot be handled by modular, memoryless and unconstrained methods. More broadly, the real challenge is to cope with $P^{i\rightarrow j}$ for $i<j$.

\vspace{2mm}
\noindent
Overall, ideally, one would like the merging of the modules to be binary, and only consider the output of the module dedicated to the task of the sample at hand, during inference. This means being able to identify the task of the sample at hand. So, in this sense, answering the third question (merging) boils down to answer the first question (distributing), which cannot be answered under our hypotheses. In the absence of a way to circumvent this issue, approaches have no choice but to perform trade-offs between the tasks, which causes their limited performances.



%\section{Additional remarks and opening}
% GradMax
% Usecases
% DANN



\section{Conclusion}


Overall, in this thesis, in addition to explore and develop a method to leverage artificial neurogenesis in order to tackle continual learning, we did an extensive and critical review of a branch of literature of continual learning. This critic included both a geometrical and a combinatorial argument which both lead to the conclusion that many regularization-based and architectural-based approaches are inherently limited to perform trade-offs, which explains the comparative success of other approaches focused on memory and emerging features. As a result, we recommend moving away from modular methods that neither utilize any form of memory from the past tasks nor leverage architectural constraints to enable the emergence and manipulation of task's features. 

\vspace{2mm}
\noindent
Additionally, we acknowledged that the manipulation of hyperparameters in the context of continual learning is subtle, and we encourage more care in their management. Finally, along with the study of hyperparameters optimizations, the introduction of a validation paradigm in continual learning led us to identify a form of overfitting on benchmarks, and ultimately led us to the conclusion that the performances of several continual learning approaches, including ours, are highly dependent on the choice of hyperparameters, which comfirms that trade-offs are a core component of the success of these approaches. 



\section{Acknowledgment}



I would like to express my deepest gratitude to Melika Payvand for offering the great opportunity to do my Master thesis in her team. Her guidance and encouragement throughout this thesis have been crucial in its completion. I am also thankful Karthik Raghunathan, whose idea GroHess was. Numerous technical discussions with him allowed me develop this method in its entirety. I also want to warmly thank my entire team, whose feedback has truly shaped my thesis into what it is today and who made the time spent with them an enjoyable period that I will remember. I am also grateful to the whole Institute of Neuroinformatic (INI) for welcoming me. Beyond my thesis, the various fascinating discussions I had with members of INI broadened my perspective on Research, as a general concept, and helped me to clarify my own specific interests. A special thank you goes to Guillaume Charpiat for accepting to be my referent professor, and providing meaningful support every time I needed it. Lastly, I would like to express my deepest appreciation to my parents, for their unwavering support throughout this thesis and throughout my entire studies. Without them, this journey would not have been possible.



%\nocite{*}
\printbibliography



\end{document}








\vspace{2mm}
\noindent
More boradly, the challenge is to deal with $P^{i\rightarrow j}$ for $j \neq i$, which requires to learn data from task $i$ without being polluted by data from task $j$. Regularization approaches such as EWC claim to solve pollution of task $j$ over task $i$ $(P^{i\rightarrow j}=0)$ for $j<i$. While they can't really overcome it, they can at least mitigate it by penalizing the distance between the weights of the model after training on task $i$ and the weights of the model after training on task $j$. However, they do not tackle the minimization of $P^{i\rightarrow j}$ for $j>i$. But they have a good reason no to tackle it : this is a fundamental limitation of this familly approaches, as it is inherently not possible to deal with it. Indeed, without contraints or some form of memory of the previous tasks, when provided a sample from task $i$, the model has no discrimination power of weights updated for task $j$ from the ones the approach identified as specific for task $i$ when training on task $j$, for $j>i$. So it can't know how to react to a sample from task $i$ after training on task $j>i$.


\vspace{2mm}
\noindent
Conversely, allowing some form of memorization of previous data distribution may enable to discriminate between samples features. This is has been attempted through replay memory approaches \cite{replay1}, \cite{replay2} and rehearsal methods \cite{rehearsal}. Alternatively, it can also be achieved through gating mechanisms such as mixture of expert models \cite{moe}, \cite{dmoe}. These approaches are not memoryless. However, in memoryless approaches we have no direct control over the way the neurons specialized to task $j$ will react when fed with samples from task $i<j$. So the very last hopes without memory is that the weights are such that they implicitly encode information about the task's specificities. This can be performed with constrained models, such as CNN, through which one could enforce convolution kernels associated to one task to be orthogonal to the convolution kernels associated to another task. And with additional contraints, this would prevent the model from using the same features for two different tasks while selecting for the most relevant features for each task. But, we considere unconstrained approaches, so the model has no reason to encourage such behavior.















\subsection{Continual learning usecases}

We already argued that continual learning is not a well-defined problem. In this section, we will question the very fundamental assumption of continual learning. and it is not clear what are the usecases of continual learning.

Here is a list each and every usecase of continual learning :
\begin{itemize}
    \item Adapt to new data quickly https://neptune.ai/blog/continual-learning-methods-and-application
        \begin{itemize}
            \item Bank fraud (want to adapt quickly to new method)
        \end{itemize}
    \item A model needs to be personalized https://neptune.ai/blog/continual-learning-methods-and-application
        \begin{itemize}
            \item Lets say you maintain a document classification pipeline, and each of your many users has slightly different data to be processedfor example, documents with different vocabulary and writing styles. With continual learning, you can use each document to automatically retrain models, gradually adjusting it to the data the user uploads to the system.
        \end{itemize}
\end{itemize}

Remarks :
\begin{itemize}
    \item For bank fraud, we don't have enough knowledge to formulate objections.
    \item Model personalization sounds like finetuining, not continual learning... does it make sense to try to train a single model that we want to use on every client ? Isn't it better to do it in the LoRA fashion, and fine-tune a module that we store with the  client's data ?
\end{itemize}




\vspace{1mm}
\noindent
Overcomplexification : In AFEC: Active Forgetting of Negative Transfer in Continual Learning, they introduce a new term in the loss, with a new hyperparameter "while the forgetting factor regulates a penalty to selectively merge the main network parameters with the expanded parameters, so as to learn a better overall representation of both the old tasks and the new task.", so they are trying to mitigate the mitigation made by a regularization method such as EWC. Additionnaly, they hope "to learn a better overall representation of both the old tasks and the new task", which has no reason to happen without enforcing such behavior through constraints, which they don't do.





%========================================================
% Old cheated HPO
\begin{table}[h!]
    \centering
    \begin{tabular}{cc}
        \begin{tabular}{c|ccc|}
            \hline
            Data from & $T_1$ & $\cdots$ & $T_m$ \\ \hline
            \multirow{3}{*}{HP at hand} & \multicolumn{3}{c|}{$h_1$} \\
            & \multicolumn{3}{c|}{$\vdots$} \\
            & \multicolumn{3}{c|}{$h_m$} \\ \hline
        \end{tabular}
        &
        \hspace{-5mm}
        \begin{tabular}{c}
                                                    \\
                                                    \\
                                                    \\
            \hspace{-1.5mm}\footnotesize{$HPO$}\\
        \end{tabular}
    \end{tabular}
    \caption{Cheated HPO : $O(1)$ parameters}
    \label{tab:cheated_hpo}
\end{table}




%========================================================
% Cheated HPO
\begin{table}[h!]
    \centering
    \begin{tabular}{cc}
        \begin{tabular}{c|ccc|}
            \hline
            Data from & $T_1$ & $\cdots$ & $T_m$ \\ \hline
            HP at hand & \multicolumn{3}{c|}{$\lambda_1$} \\\hline
        \end{tabular}
        &
        \hspace{-5mm}
        \begin{tabular}{c}
                                                    \\
                                                    \\
                                                    \\
            \hspace{-1.5mm}\footnotesize{$HPO$}\\
        \end{tabular}
    \end{tabular}
    \caption{Cheated HPO : $O(1)$ parameters}
    \label{tab:cheated_hpo1}
\end{table}


%========================================================
% Old greedy HPO
\begin{table*}[h!]
    \centering
    \begin{tabular}{cccccc}
        \begin{tabular}{|c|}
            \hline
            $T_1$    \\\hline
            $h_1^1$  \\
            $\vdots$ \\
            $h_m^1$  \\\hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                \\
            \hspace{2mm} $\longrightarrow$      \\
                                                \\
            \hspace{-3mm}\footnotesize{$HPO_1$} \\
        \end{tabular}
        &
        \hspace{-4mm}
        \begin{tabular}{|c|}
            \hline
            $T_2$    \\\hline
            $h_1^2$  \\
            $\vdots$ \\
            $h_m^2$  \\\hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                                          \\
            \hspace{2mm} $\longrightarrow$ \ldots $\longrightarrow$       \\
                                                                          \\
            \hspace{-14mm}\footnotesize{$HPO_2$}                          \\
        \end{tabular}
        &
        \hspace{-4mm}
        \begin{tabular}{|c|}
            \hline
            $T_n$    \\\hline
            $h_1^n$  \\
            $\vdots$ \\
            $h_m^n$  \\\hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                 \\
                                                 \\
                                                 \\
            \hspace{-1.5mm}\footnotesize{$HPO_n$}\\
        \end{tabular}
    \end{tabular}
    \caption{Greedy HPO : $O(n)$ to $O(n^2)$ parameters}
    \label{tab:greedy_hpo}
\end{table*}


%========================================================
% Greedy HPO n
\begin{table*}[h!]
    \centering
    \begin{tabular}{cccccc}
        \begin{tabular}{|c|}
            \hline
            $T_1$    \\\hline
            $\emptyset$  \\\hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                \\
            \hspace{2mm} $\longrightarrow$      \\
                                                \\
            \hspace{-3mm}\footnotesize{$HPO_1$} \\
        \end{tabular}
        &
        \hspace{-4mm}
        \begin{tabular}{|c|}
            \hline
            $T_2$    \\\hline
            $\lambda_2$  \\\hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                                          \\
            \hspace{2mm} $\longrightarrow$ \ldots $\longrightarrow$       \\
                                                                          \\
            \hspace{-14mm}\footnotesize{$HPO_2$}                          \\
        \end{tabular}
        &
        \hspace{-4mm}
        \begin{tabular}{|c|}
            \hline
            $T_n$    \\\hline
            $\lambda_n$  \\\hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                 \\
                                                 \\
                                                 \\
            \hspace{-1.5mm}\footnotesize{$HPO_n$}\\
        \end{tabular}
    \end{tabular}
    \caption{Greedy HPO : $O(n)$ parameters}
    \label{tab:greedy_hpo1}
\end{table*}






%========================================================
% Greedy HPO n^2
\begin{table*}[h!]
    \centering
    \begin{tabular}{cccccccc}
        \begin{tabular}{|c|}
            \hline
            $T_1$    \\\hline
            $\emptyset$  \\\hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                \\
            \hspace{2mm} $\longrightarrow$      \\
                                                \\
            \hspace{-3mm}\footnotesize{$HPO_1$} \\
        \end{tabular}
        &
        \hspace{-4mm}
        \begin{tabular}{|c|}
            \hline
            $T_2$    \\ \hline
            $\lambda_{2,1}$ \\ \hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                \\
            \hspace{2mm} $\longrightarrow$      \\
                                                \\
            \hspace{-3mm}\footnotesize{$HPO_2$} \\
        \end{tabular}
        &
        \hspace{-4mm}
        \begin{tabular}{|c|}
            \hline
            $T_3$    \\ \hline
            $\lambda_{3,1}$ \\
            $\lambda_{3,2}$ \\ \hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                                          \\
            \hspace{2mm} $\longrightarrow$ \ldots $\longrightarrow$       \\
                                                                          \\
            \hspace{-14mm}\footnotesize{$HPO_3$}                          \\
        \end{tabular}
        &
        \hspace{-4mm}
        \begin{tabular}{|c|}
            \hline
            $T_n$    \\\hline
            $\lambda_{n,1}$  \\
            $\vdots$ \\
            $\lambda_{n,n-1}$  \\\hline
        \end{tabular}
        &
        \hspace{-6mm}
        \begin{tabular}{c}
                                                 \\
                                                 \\
                                                 \\
            \hspace{-1.5mm}\footnotesize{$HPO_n$}\\
        \end{tabular}
    \end{tabular}
    \caption{Greedy HPO : $O(n^2)$ parameters}
    \label{tab:greedy_hpo2}
\end{table*}